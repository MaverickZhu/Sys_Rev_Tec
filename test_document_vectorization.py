#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£å‘é‡åŒ–åŠŸèƒ½æµ‹è¯•
æµ‹è¯•æ™ºèƒ½æ–‡æ¡£å¤„ç†çš„æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from app.utils.ai_integration import AIIntegrationService
from app.utils.text_processing import TextProcessor
from app.utils.vector_service import VectorService
from app.core.config import settings

async def test_document_vectorization():
    """
    æµ‹è¯•æ–‡æ¡£å‘é‡åŒ–æµç¨‹
    """
    print("=== æµ‹è¯•æ–‡æ¡£å‘é‡åŒ–åŠŸèƒ½ ===")
    
    # åˆå§‹åŒ–æœåŠ¡
    ai_service = AIIntegrationService()
    text_processor = TextProcessor()
    vector_service = VectorService()
    
    # æµ‹è¯•æ–‡æ¡£å†…å®¹
    test_document = """
    æ”¿åºœé‡‡è´­é¡¹ç›®å®¡æŸ¥æŠ¥å‘Š
    
    é¡¹ç›®åç§°ï¼šåŠå…¬è®¾å¤‡é‡‡è´­é¡¹ç›®
    é‡‡è´­å•ä½ï¼šæŸå¸‚æ”¿åºœåŠå…¬å®¤
    é‡‡è´­é‡‘é¢ï¼š500ä¸‡å…ƒ
    
    é¡¹ç›®æ¦‚è¿°ï¼š
    æœ¬é¡¹ç›®æ—¨åœ¨ä¸ºå¸‚æ”¿åºœåŠå…¬å®¤é‡‡è´­ä¸€æ‰¹ç°ä»£åŒ–åŠå…¬è®¾å¤‡ï¼ŒåŒ…æ‹¬è®¡ç®—æœºã€æ‰“å°æœºã€å¤å°æœºç­‰ã€‚
    é‡‡è´­è¿‡ç¨‹ä¸¥æ ¼æŒ‰ç…§ã€Šæ”¿åºœé‡‡è´­æ³•ã€‹ç›¸å…³è§„å®šæ‰§è¡Œï¼Œç¡®ä¿å…¬å¼€ã€å…¬å¹³ã€å…¬æ­£ã€‚
    
    æŠ€æœ¯è¦æ±‚ï¼š
    1. è®¡ç®—æœºé…ç½®ï¼šIntel i7å¤„ç†å™¨ï¼Œ16GBå†…å­˜ï¼Œ512GB SSDç¡¬ç›˜
    2. æ‰“å°æœºè¦æ±‚ï¼šæ¿€å…‰æ‰“å°ï¼ŒåŒé¢æ‰“å°åŠŸèƒ½ï¼Œç½‘ç»œè¿æ¥
    3. å¤å°æœºè¦æ±‚ï¼šå¤šåŠŸèƒ½ä¸€ä½“æœºï¼Œæ‰«æã€å¤å°ã€ä¼ çœŸåŠŸèƒ½
    
    ä¾›åº”å•†è¯„ä¼°ï¼š
    ç»è¿‡å…¬å¼€æ‹›æ ‡ï¼Œå…±æœ‰5å®¶ä¾›åº”å•†å‚ä¸æŠ•æ ‡ã€‚è¯„æ ‡å§”å‘˜ä¼šæŒ‰ç…§æŠ€æœ¯æ ‡å‡†ã€ä»·æ ¼å› ç´ ã€
    æœåŠ¡èƒ½åŠ›ç­‰æ–¹é¢è¿›è¡Œç»¼åˆè¯„ä¼°ï¼Œæœ€ç»ˆç¡®å®šä¸­æ ‡ä¾›åº”å•†ã€‚
    
    é£é™©è¯„ä¼°ï¼š
    1. æŠ€æœ¯é£é™©ï¼šè®¾å¤‡å…¼å®¹æ€§é—®é¢˜
    2. ä¾›åº”é£é™©ï¼šä¾›åº”å•†å±¥çº¦èƒ½åŠ›
    3. ä»·æ ¼é£é™©ï¼šå¸‚åœºä»·æ ¼æ³¢åŠ¨
    
    åˆè§„æ€§æ£€æŸ¥ï¼š
    é¡¹ç›®é‡‡è´­æµç¨‹ç¬¦åˆç›¸å…³æ³•å¾‹æ³•è§„è¦æ±‚ï¼Œèµ„é‡‘æ¥æºåˆæ³•ï¼Œé¢„ç®—å®¡æ‰¹å®Œæ•´ã€‚
    """
    
    try:
        print("\n1. æ–‡æœ¬é¢„å¤„ç†...")
        # æ–‡æœ¬æ¸…ç†å’Œé¢„å¤„ç†
        cleaned_text = text_processor.clean_text(test_document)
        print(f"æ¸…ç†åæ–‡æœ¬é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
        
        # æ–‡æœ¬åˆ†å—
        chunks = text_processor.chunk_text(cleaned_text, chunk_size=200, chunk_overlap=50)
        print(f"æ–‡æœ¬åˆ†å—æ•°é‡: {len(chunks)}")
        
        # æå–å…³é”®è¯
        keywords = text_processor.extract_keywords(cleaned_text)
        print(f"æå–å…³é”®è¯: {keywords[:10]}")
        
        print("\n2. æ–‡æ¡£å‘é‡åŒ–...")
        # åˆ›å»ºæ¨¡æ‹Ÿæ–‡æ¡£å¯¹è±¡
        mock_document = type('MockDocument', (), {
            'id': 'test_doc_001',
            'title': 'åŠå…¬è®¾å¤‡é‡‡è´­é¡¹ç›®',
            'content': test_document,
            'extracted_text': cleaned_text,
            'ocr_text': None
        })()
        
        # æ–‡æ¡£å‘é‡åŒ–
        vectorization_result = await ai_service.vectorize_document(
            document=mock_document,
            model_name="bge-m3:latest",
            chunk_size=200,
            chunk_overlap=50
        )
        
        print(f"æ–‡æ¡£ID: {vectorization_result.get('document_id')}")
        print(f"å‘é‡ç»´åº¦: {vectorization_result.get('embedding_dimension')}")
        print(f"æ–‡æœ¬å—æ•°é‡: {vectorization_result.get('total_chunks')}")
        
        print("\n3. æ™ºèƒ½åˆ†æ...")
        # æ–‡æ¡£æ™ºèƒ½åˆ†æ
        analysis_result = await ai_service.analyze_document(
            document=mock_document,
            analysis_types=["summary", "keywords", "classification", "risk_assessment", "sentiment_analysis"]
        )
        
        print(f"åˆ†æç»“æœç±»å‹: {list(analysis_result.keys())}")
        if 'classification' in analysis_result:
            print(f"æ–‡æ¡£åˆ†ç±»: {analysis_result.get('classification')}")
        if 'risk_assessment' in analysis_result:
            print(f"é£é™©è¯„ä¼°: {analysis_result.get('risk_assessment')}")
        if 'sentiment_analysis' in analysis_result:
            print(f"æƒ…æ„Ÿåˆ†æ: {analysis_result.get('sentiment_analysis')}")
        if 'keywords' in analysis_result:
            print(f"å…³é”®è¯æ•°é‡: {len(analysis_result.get('keywords', []))}")
        
        print("\n4. è¯­ä¹‰æœç´¢æµ‹è¯•...")
        # æµ‹è¯•è¯­ä¹‰æœç´¢
        search_queries = [
            "åŠå…¬è®¾å¤‡é‡‡è´­",
            "é£é™©è¯„ä¼°",
            "ä¾›åº”å•†è¯„ä¼°",
            "åˆè§„æ€§æ£€æŸ¥"
        ]
        
        for query in search_queries:
            print(f"\næœç´¢æŸ¥è¯¢: '{query}'")
            # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æ•°æ®åº“ä¼šè¯
            from unittest.mock import MagicMock
            mock_db = MagicMock()
            
            search_results = await vector_service.semantic_search(
                db=mock_db,
                query=query,
                max_results=3,
                similarity_threshold=0.5
            )
            
            if search_results:
                for i, result in enumerate(search_results[:2], 1):
                    print(f"  ç»“æœ{i}: ç›¸ä¼¼åº¦={result.get('score', 0):.3f}")
                    print(f"    å†…å®¹é¢„è§ˆ: {result.get('content', '')[:100]}...")
            else:
                print("  æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        
        print("\n5. å‘é‡å­˜å‚¨æµ‹è¯•...")
        # æµ‹è¯•å‘é‡å­˜å‚¨ - ä½¿ç”¨ update_document_vectors æ–¹æ³•
        from unittest.mock import MagicMock
        mock_db = MagicMock()
        
        try:
            # æµ‹è¯•æ›´æ–°æ–‡æ¡£å‘é‡
            update_result = await vector_service.update_document_vectors(
                db=mock_db,
                document_id=1,  # ä½¿ç”¨æµ‹è¯•æ–‡æ¡£ID
                force_update=True
            )
            print(f"å‘é‡æ›´æ–°ç»“æœ: {update_result}")
        except Exception as e:
            print(f"å‘é‡å­˜å‚¨æµ‹è¯•è·³è¿‡: {str(e)}")
            print("æ³¨æ„: å‘é‡å­˜å‚¨éœ€è¦çœŸå®çš„æ•°æ®åº“è¿æ¥")
        
        print("\nâœ… æ–‡æ¡£å‘é‡åŒ–åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

async def test_batch_processing():
    """
    æµ‹è¯•æ‰¹é‡æ–‡æ¡£å¤„ç†
    """
    print("\n=== æµ‹è¯•æ‰¹é‡æ–‡æ¡£å¤„ç† ===")
    
    ai_service = AIIntegrationService()
    
    # æ¨¡æ‹Ÿå¤šä¸ªæ–‡æ¡£
    test_documents = [
        {
            "id": "doc_001",
            "title": "é‡‡è´­åˆåŒå®¡æŸ¥",
            "content": "æœ¬åˆåŒä¸ºæ”¿åºœé‡‡è´­åˆåŒï¼Œæ¶‰åŠåŠå…¬ç”¨å“é‡‡è´­ï¼Œé‡‘é¢100ä¸‡å…ƒã€‚åˆåŒæ¡æ¬¾å®Œæ•´ï¼Œç¬¦åˆç›¸å…³æ³•è§„è¦æ±‚ã€‚"
        },
        {
            "id": "doc_002", 
            "title": "ä¾›åº”å•†èµ„è´¨å®¡æŸ¥",
            "content": "ä¾›åº”å•†å…·å¤‡ç›¸åº”èµ„è´¨è¯ä¹¦ï¼Œè´¢åŠ¡çŠ¶å†µè‰¯å¥½ï¼Œå±¥çº¦èƒ½åŠ›å¼ºã€‚å»ºè®®é€šè¿‡èµ„è´¨å®¡æŸ¥ã€‚"
        },
        {
            "id": "doc_003",
            "title": "é¡¹ç›®éªŒæ”¶æŠ¥å‘Š",
            "content": "é¡¹ç›®æŒ‰æœŸå®Œæˆï¼Œè®¾å¤‡å®‰è£…è°ƒè¯•æ­£å¸¸ï¼Œç”¨æˆ·åŸ¹è®­åˆ°ä½ã€‚éªŒæ”¶åˆæ ¼ï¼Œå»ºè®®åŠç†ç»“ç®—æ‰‹ç»­ã€‚"
        }
    ]
    
    try:
        print(f"\nå¼€å§‹æ‰¹é‡å¤„ç† {len(test_documents)} ä¸ªæ–‡æ¡£...")
        
        # æ‰¹é‡å‘é‡åŒ–
        batch_results = []
        for doc in test_documents:
            print(f"\nå¤„ç†æ–‡æ¡£: {doc['title']}")
            
            # åˆ›å»ºæ¨¡æ‹Ÿæ–‡æ¡£å¯¹è±¡
            mock_doc = type('MockDocument', (), {
                'id': doc["id"],
                'title': doc["title"],
                'content': doc["content"],
                'extracted_text': doc["content"],
                'ocr_text': None
            })()
            
            result = await ai_service.vectorize_document(
                document=mock_doc,
                model_name="bge-m3:latest",
                chunk_size=200,
                chunk_overlap=50
            )
            
            batch_results.append({
                "document_id": doc["id"],
                "title": doc["title"],
                "document_id_result": result.get("document_id"),
                "chunk_count": result.get("total_chunks", 0)
            })
            
            print(f"  æ–‡æ¡£ID: {result.get('document_id')}")
            print(f"  å—æ•°: {result.get('total_chunks', 0)}")
        
        print("\næ‰¹é‡å¤„ç†ç»“æœæ±‡æ€»:")
        successful = sum(1 for r in batch_results if r["document_id_result"] is not None)
        total_chunks = sum(r["chunk_count"] for r in batch_results)
        
        print(f"  æˆåŠŸå¤„ç†: {successful}/{len(test_documents)} ä¸ªæ–‡æ¡£")
        print(f"  æ€»æ–‡æœ¬å—: {total_chunks} ä¸ª")
        
        print("\nâœ… æ‰¹é‡æ–‡æ¡£å¤„ç†æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

async def main():
    """
    ä¸»æµ‹è¯•å‡½æ•°
    """
    print("å¼€å§‹æ™ºèƒ½æ–‡æ¡£å¤„ç†åŠŸèƒ½æµ‹è¯•")
    print(f"é…ç½®ä¿¡æ¯:")
    print(f"  é»˜è®¤å‘é‡å­˜å‚¨: {settings.DEFAULT_VECTOR_STORE}")
    print(f"  OpenAI API: {'å·²é…ç½®' if settings.OPENAI_API_KEY else 'æœªé…ç½®'}")
    print(f"  Ollama URL: {settings.OLLAMA_BASE_URL or 'æœªé…ç½®'}")
    
    # è¿è¡Œæµ‹è¯•
    test_results = []
    
    # æµ‹è¯•1: æ–‡æ¡£å‘é‡åŒ–
    result1 = await test_document_vectorization()
    test_results.append(("æ–‡æ¡£å‘é‡åŒ–", result1))
    
    # æµ‹è¯•2: æ‰¹é‡å¤„ç†
    result2 = await test_batch_processing()
    test_results.append(("æ‰¹é‡å¤„ç†", result2))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "="*50)
    print("æ™ºèƒ½æ–‡æ¡£å¤„ç†åŠŸèƒ½æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*50)
    
    for test_name, success in test_results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    all_passed = all(result for _, result in test_results)
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ™ºèƒ½æ–‡æ¡£å¤„ç†åŠŸèƒ½å·²å°±ç»ªã€‚")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    
    return all_passed

if __name__ == "__main__":
    asyncio.run(main())