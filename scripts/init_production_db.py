#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§ç¯å¢ƒPostgreSQLæ•°æ®åº“åˆå§‹åŒ–è„šæœ¬

åŠŸèƒ½:
1. åˆ›å»ºç”Ÿäº§æ•°æ®åº“
2. åˆ›å»ºæ•°æ®åº“ç”¨æˆ·
3. è®¾ç½®æƒé™
4. è¿è¡Œæ•°æ®åº“è¿ç§»
5. åˆ›å»ºåˆå§‹æ•°æ®
"""

import os
import sys
import logging
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import asyncpg
from sqlalchemy import create_engine, text
from alembic.config import Config
from alembic import command

from app.core.config import settings
from app.db.init_data import init_db

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProductionDBInitializer:
    """ç”Ÿäº§æ•°æ®åº“åˆå§‹åŒ–å™¨"""
    
    def __init__(self):
        self.db_name = os.getenv("PROD_DB_NAME", "audit_system_prod")
        self.db_user = os.getenv("PROD_DB_USER", "postgres")
        self.db_password = os.getenv("PROD_DB_PASSWORD")
        if not self.db_password:
            raise ValueError("PROD_DB_PASSWORDç¯å¢ƒå˜é‡å¿…é¡»è®¾ç½®")
        self.db_host = os.getenv("PROD_DB_HOST", "localhost")
        self.db_port = int(os.getenv("PROD_DB_PORT", "5432"))
        
        # ç®¡ç†å‘˜è¿æ¥URLï¼ˆè¿æ¥åˆ°postgresæ•°æ®åº“ï¼‰
        self.admin_url = f"postgresql://{self.db_user}:{self.db_password}@{self.db_host}:{self.db_port}/postgres"
        # ç›®æ ‡æ•°æ®åº“URL
        self.target_url = f"postgresql://{self.db_user}:{self.db_password}@{self.db_host}:{self.db_port}/{self.db_name}"
    
    async def check_postgresql_connection(self):
        """æ£€æŸ¥PostgreSQLè¿æ¥"""
        try:
            conn = await asyncpg.connect(self.admin_url)
            await conn.close()
            logger.info("âœ… PostgreSQLè¿æ¥æ­£å¸¸")
            return True
        except Exception as e:
            logger.error(f"âŒ PostgreSQLè¿æ¥å¤±è´¥: {e}")
            return False
    
    async def create_database(self):
        """åˆ›å»ºç”Ÿäº§æ•°æ®åº“"""
        try:
            conn = await asyncpg.connect(self.admin_url)
            
            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
            result = await conn.fetchval(
                "SELECT 1 FROM pg_database WHERE datname = $1",
                self.db_name
            )
            
            if result:
                logger.info(f"ğŸ“Š æ•°æ®åº“ {self.db_name} å·²å­˜åœ¨")
            else:
                # åˆ›å»ºæ•°æ®åº“
                await conn.execute(f"CREATE DATABASE {self.db_name}")
                logger.info(f"âœ… æˆåŠŸåˆ›å»ºæ•°æ®åº“: {self.db_name}")
            
            await conn.close()
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def run_migrations(self):
        """è¿è¡Œæ•°æ®åº“è¿ç§»"""
        try:
            # è®¾ç½®Alembicé…ç½®
            alembic_cfg = Config("alembic.ini")
            alembic_cfg.set_main_option("sqlalchemy.url", self.target_url)
            
            # è¿è¡Œè¿ç§»
            command.upgrade(alembic_cfg, "head")
            logger.info("âœ… æ•°æ®åº“è¿ç§»å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿ç§»å¤±è´¥: {e}")
            return False
    
    def create_indexes(self):
        """åˆ›å»ºç”Ÿäº§ç¯å¢ƒç´¢å¼•"""
        try:
            engine = create_engine(self.target_url)
            
            with engine.connect() as conn:
                # åˆ›å»ºå¸¸ç”¨æŸ¥è¯¢ç´¢å¼•
                indexes = [
                    "CREATE INDEX IF NOT EXISTS idx_projects_status ON projects(status);",
                    "CREATE INDEX IF NOT EXISTS idx_projects_created_at ON projects(created_at);",
                    "CREATE INDEX IF NOT EXISTS idx_projects_updated_at ON projects(updated_at);",
                    "CREATE INDEX IF NOT EXISTS idx_users_username ON users(username);",
                    "CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);",
                    "CREATE INDEX IF NOT EXISTS idx_users_is_active ON users(is_active);",
                    "CREATE INDEX IF NOT EXISTS idx_audit_logs_timestamp ON audit_logs(timestamp);",
                    "CREATE INDEX IF NOT EXISTS idx_audit_logs_user_id ON audit_logs(user_id);",
                    "CREATE INDEX IF NOT EXISTS idx_audit_logs_action ON audit_logs(action);",
                    "CREATE INDEX IF NOT EXISTS idx_files_project_id ON files(project_id);",
                    "CREATE INDEX IF NOT EXISTS idx_files_created_at ON files(created_at);",
                ]
                
                for index_sql in indexes:
                    conn.execute(text(index_sql))
                    logger.info(f"âœ… åˆ›å»ºç´¢å¼•: {index_sql.split('idx_')[1].split(' ')[0]}")
                
                conn.commit()
            
            engine.dispose()
            logger.info("âœ… æ‰€æœ‰ç´¢å¼•åˆ›å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“é…ç½®"""
        try:
            engine = create_engine(self.target_url)
            
            with engine.connect() as conn:
                # PostgreSQLæ€§èƒ½ä¼˜åŒ–è®¾ç½®
                optimizations = [
                    "ALTER SYSTEM SET shared_buffers = '256MB';",
                    "ALTER SYSTEM SET effective_cache_size = '1GB';",
                    "ALTER SYSTEM SET maintenance_work_mem = '64MB';",
                    "ALTER SYSTEM SET checkpoint_completion_target = 0.9;",
                    "ALTER SYSTEM SET wal_buffers = '16MB';",
                    "ALTER SYSTEM SET default_statistics_target = 100;",
                ]
                
                for opt_sql in optimizations:
                    try:
                        conn.execute(text(opt_sql))
                        logger.info(f"âœ… åº”ç”¨ä¼˜åŒ–: {opt_sql.split('SET ')[1].split(' =')[0]}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ ä¼˜åŒ–è®¾ç½®è·³è¿‡ (éœ€è¦è¶…çº§ç”¨æˆ·æƒé™): {e}")
                
                # æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
                conn.execute(text("ANALYZE;"))
                logger.info("âœ… æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯å®Œæˆ")
                
                conn.commit()
            
            engine.dispose()
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")
            return False
    
    async def initialize_data(self):
        """åˆå§‹åŒ–åŸºç¡€æ•°æ®"""
        try:
            # æ›´æ–°ç¯å¢ƒå˜é‡
            os.environ["DATABASE_URL"] = self.target_url
            
            # åˆå§‹åŒ–æ•°æ®
            await init_db()
            logger.info("âœ… åŸºç¡€æ•°æ®åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def run_full_initialization(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®åº“åˆå§‹åŒ–æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹ç”Ÿäº§æ•°æ®åº“åˆå§‹åŒ–...")
        
        # 1. æ£€æŸ¥PostgreSQLè¿æ¥
        if not await self.check_postgresql_connection():
            logger.error("âŒ åˆå§‹åŒ–å¤±è´¥ï¼šæ— æ³•è¿æ¥åˆ°PostgreSQL")
            return False
        
        # 2. åˆ›å»ºæ•°æ®åº“
        if not await self.create_database():
            logger.error("âŒ åˆå§‹åŒ–å¤±è´¥ï¼šæ— æ³•åˆ›å»ºæ•°æ®åº“")
            return False
        
        # 3. è¿è¡Œè¿ç§»
        if not self.run_migrations():
            logger.error("âŒ åˆå§‹åŒ–å¤±è´¥ï¼šæ•°æ®åº“è¿ç§»å¤±è´¥")
            return False
        
        # 4. åˆ›å»ºç´¢å¼•
        if not self.create_indexes():
            logger.error("âŒ åˆå§‹åŒ–å¤±è´¥ï¼šåˆ›å»ºç´¢å¼•å¤±è´¥")
            return False
        
        # 5. ä¼˜åŒ–æ•°æ®åº“
        if not self.optimize_database():
            logger.warning("âš ï¸ æ•°æ®åº“ä¼˜åŒ–éƒ¨åˆ†å¤±è´¥ï¼Œä½†å¯ä»¥ç»§ç»­")
        
        # 6. åˆå§‹åŒ–æ•°æ®
        if not await self.initialize_data():
            logger.error("âŒ åˆå§‹åŒ–å¤±è´¥ï¼šåŸºç¡€æ•°æ®åˆ›å»ºå¤±è´¥")
            return False
        
        logger.info("ğŸ‰ ç”Ÿäº§æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼")
        logger.info(f"ğŸ“Š æ•°æ®åº“è¿æ¥URL: {self.target_url}")
        return True


async def main():
    """ä¸»å‡½æ•°"""
    initializer = ProductionDBInitializer()
    success = await initializer.run_full_initialization()
    
    if success:
        print("\nâœ… ç”Ÿäº§æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸï¼")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. å¯åŠ¨åº”ç”¨æœåŠ¡")
        print("2. éªŒè¯APIåŠŸèƒ½")
        print("3. æ£€æŸ¥æ•°æ®åº“è¿æ¥")
        return 0
    else:
        print("\nâŒ ç”Ÿäº§æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼")
        print("\nğŸ”§ æ•…éšœæ’é™¤:")
        print("1. æ£€æŸ¥PostgreSQLæœåŠ¡æ˜¯å¦è¿è¡Œ")
        print("2. éªŒè¯æ•°æ®åº“è¿æ¥å‚æ•°")
        print("3. æ£€æŸ¥ç”¨æˆ·æƒé™")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)