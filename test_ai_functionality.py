#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•AIåŠŸèƒ½çš„è„šæœ¬
éªŒè¯æ–‡æ¡£å‘é‡åŒ–ã€è¯­ä¹‰æœç´¢ç­‰åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.utils.ai_integration import AIIntegrationService
from app.utils.vector_service import VectorService
from app.utils.text_processing import TextProcessor
from app.models.document import Document
from app.db.session import SessionLocal
from app.crud.crud_document import document as crud_document


async def test_text_processing():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†åŠŸèƒ½"""
    print("\n=== æµ‹è¯•æ–‡æœ¬å¤„ç†åŠŸèƒ½ ===")
    
    processor = TextProcessor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯AIåŠŸèƒ½çš„æ­£å¸¸å·¥ä½œã€‚
    æ–‡æ¡£åŒ…å«å¤šä¸ªæ®µè½ï¼Œæ¯ä¸ªæ®µè½éƒ½æœ‰ä¸åŒçš„å†…å®¹ã€‚
    æˆ‘ä»¬éœ€è¦æµ‹è¯•æ–‡æœ¬åˆ†å—ã€å…³é”®è¯æå–ã€æ–‡æœ¬æ¸…ç†ç­‰åŠŸèƒ½ã€‚
    
    ç¬¬äºŒæ®µè½åŒ…å«ä¸€äº›æŠ€æœ¯æœ¯è¯­ï¼Œæ¯”å¦‚APIã€æ•°æ®åº“ã€å‘é‡åŒ–ç­‰ã€‚
    è¿™äº›æœ¯è¯­åº”è¯¥è¢«æ­£ç¡®è¯†åˆ«å’Œå¤„ç†ã€‚
    
    æœ€åä¸€æ®µåŒ…å«ä¸€äº›æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ï¼š123, @#$%, æµ‹è¯•@example.com
    """
    
    # æµ‹è¯•æ–‡æœ¬æ¸…ç†
    cleaned_text = processor.clean_text(test_text)
    print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(test_text)}")
    print(f"æ¸…ç†åæ–‡æœ¬é•¿åº¦: {len(cleaned_text)}")
    print(f"æ¸…ç†åæ–‡æœ¬é¢„è§ˆ: {cleaned_text[:100]}...")
    
    # æµ‹è¯•æ–‡æœ¬åˆ†å—
    chunks = processor.chunk_text(cleaned_text, chunk_size=100, chunk_overlap=20)
    print(f"\nåˆ†å—æ•°é‡: {len(chunks)}")
    for i, chunk in enumerate(chunks[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªåˆ†å—
        print(f"åˆ†å— {i+1}: {chunk['text'][:50]}...")
    
    # æµ‹è¯•å…³é”®è¯æå–
    keywords = processor.extract_keywords(cleaned_text, max_keywords=10)
    print(f"\næå–çš„å…³é”®è¯: {keywords}")
    
    # æµ‹è¯•æ–‡æœ¬ç»Ÿè®¡
    stats = processor.get_text_statistics(cleaned_text)
    print(f"\næ–‡æœ¬ç»Ÿè®¡: {stats}")
    
    return True


async def test_ai_integration():
    """æµ‹è¯•AIé›†æˆæœåŠ¡"""
    print("\n=== æµ‹è¯•AIé›†æˆæœåŠ¡ ===")
    
    ai_service = AIIntegrationService()
    
    # æµ‹è¯•æ–‡æœ¬åµŒå…¥ï¼ˆæ¨¡æ‹Ÿï¼‰
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯å‘é‡åŒ–åŠŸèƒ½ã€‚"
    
    try:
        # ç”±äºæ²¡æœ‰çœŸå®çš„AIæ¨¡å‹ï¼Œè¿™é‡Œä¼šä½¿ç”¨æ¨¡æ‹Ÿçš„åµŒå…¥
        embedding = await ai_service._get_text_embedding(test_text)
        print(f"æ–‡æœ¬åµŒå…¥ç»´åº¦: {len(embedding)}")
        print(f"åµŒå…¥å‘é‡å‰5ä¸ªå€¼: {embedding[:5]}")
        
        # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
        embedding2 = await ai_service._get_text_embedding("è¿™æ˜¯å¦ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚")
        similarity = ai_service._calculate_cosine_similarity(embedding, embedding2)
        print(f"ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦: {similarity:.4f}")
        
        return True
        
    except Exception as e:
        print(f"AIé›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")
        return False


async def test_vector_service():
    """æµ‹è¯•å‘é‡æœåŠ¡"""
    print("\n=== æµ‹è¯•å‘é‡æœåŠ¡ ===")
    
    vector_service = VectorService()
    
    # æµ‹è¯•å‘é‡å­˜å‚¨åˆå§‹åŒ–
    print(f"å¯ç”¨çš„å‘é‡å­˜å‚¨: {list(vector_service.vector_stores.keys())}")
    print(f"é»˜è®¤å‘é‡å­˜å‚¨: {vector_service.default_store}")
    
    # æµ‹è¯•è¯­ä¹‰æœç´¢ï¼ˆæ¨¡æ‹Ÿï¼‰
    try:
        # ç”±äºæ²¡æœ‰çœŸå®çš„æ•°æ®åº“æ•°æ®ï¼Œè¿™é‡Œåªæµ‹è¯•æœåŠ¡åˆå§‹åŒ–
        print("å‘é‡æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"å‘é‡æœåŠ¡æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


async def test_database_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    print("\n=== æµ‹è¯•æ•°æ®åº“è¿æ¥ ===")
    
    try:
        db = SessionLocal()
        
        # æµ‹è¯•æŸ¥è¯¢æ–‡æ¡£
        documents = crud_document.get_multi(db, limit=5)
        print(f"æ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°é‡ï¼ˆå‰5ä¸ªï¼‰: {len(documents)}")
        
        for doc in documents:
            print(f"æ–‡æ¡£ID: {doc.id}, æ ‡é¢˜: {doc.title}, çŠ¶æ€: {doc.status}")
        
        db.close()
        return True
        
    except Exception as e:
        print(f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹AIåŠŸèƒ½æµ‹è¯•...")
    
    test_results = []
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_results.append(("æ–‡æœ¬å¤„ç†", await test_text_processing()))
    test_results.append(("AIé›†æˆæœåŠ¡", await test_ai_integration()))
    test_results.append(("å‘é‡æœåŠ¡", await test_vector_service()))
    test_results.append(("æ•°æ®åº“è¿æ¥", await test_database_connection()))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
    all_passed = True
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if not result:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AIåŠŸèƒ½åŸºç¡€æ¶æ„æ­£å¸¸å·¥ä½œã€‚")
        print("\nä¸‹ä¸€æ­¥å¯ä»¥å¼€å§‹å®ç°å…·ä½“çš„AIåŠŸèƒ½ï¼š")
        print("1. æ–‡æ¡£å‘é‡åŒ–APIæµ‹è¯•")
        print("2. è¯­ä¹‰æœç´¢åŠŸèƒ½æµ‹è¯•")
        print("3. æ™ºèƒ½åˆ†æåŠŸèƒ½æµ‹è¯•")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ç›¸å…³é…ç½®å’Œä¾èµ–ã€‚")
    
    return all_passed


if __name__ == "__main__":
    asyncio.run(main())