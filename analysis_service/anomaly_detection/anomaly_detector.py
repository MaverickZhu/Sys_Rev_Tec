#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼‚å¸¸æ£€æµ‹å™¨
æ”¿åºœé‡‡è´­é¡¹ç›®å¼‚å¸¸æ£€æµ‹çš„æ ¸å¿ƒå¼•æ“

ä¸»è¦åŠŸèƒ½:
- å¤šç®—æ³•å¼‚å¸¸æ£€æµ‹
- æ¨¡å¼è¯†åˆ«å’Œå­¦ä¹ 
- å®æ—¶å¼‚å¸¸ç›‘æ§
- å¼‚å¸¸è¯„åˆ†å’Œåˆ†çº§
- æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ

ä½œè€…: AI Assistant
åˆ›å»ºæ—¶é—´: 2025-07-28
ç‰ˆæœ¬: 1.0.0
"""

import logging
import asyncio
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import json
import hashlib
from collections import defaultdict

# æœºå™¨å­¦ä¹ åº“
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.svm import OneClassSVM
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn.cluster import DBSCAN
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.warning("scikit-learnæœªå®‰è£…ï¼Œéƒ¨åˆ†æœºå™¨å­¦ä¹ åŠŸèƒ½å°†ä¸å¯ç”¨")

from .anomaly_models import (
    AnomalyType, AnomalySeverity, AnomalyStatus, DetectionMethod,
    AnomalyResult, AnomalyReport, AnomalyPattern, AnomalyMetrics,
    DetectionConfig, AnomalyThreshold, TimeSeriesPoint, BehaviorProfile
)


class AnomalyDetector:
    """
    å¼‚å¸¸æ£€æµ‹å™¨
    
    é›†æˆå¤šç§å¼‚å¸¸æ£€æµ‹ç®—æ³•ï¼Œæä¾›å…¨é¢çš„å¼‚å¸¸è¯†åˆ«å’Œåˆ†æèƒ½åŠ›ã€‚
    æ”¯æŒç»Ÿè®¡æ–¹æ³•ã€æœºå™¨å­¦ä¹ ç®—æ³•å’Œè‡ªå®šä¹‰è§„åˆ™ã€‚
    """
    
    def __init__(self, config: Optional[DetectionConfig] = None):
        """
        åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨
        
        Args:
            config: æ£€æµ‹é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or DetectionConfig()
        self.logger = logging.getLogger(__name__)
        
        # æ£€æµ‹å™¨ç»„ä»¶
        self.patterns: Dict[str, AnomalyPattern] = {}
        self.thresholds: Dict[str, AnomalyThreshold] = {}
        self.behavior_profiles: Dict[str, BehaviorProfile] = {}
        
        # å†å²è®°å½•
        self.detection_history: List[AnomalyReport] = []
        self.anomaly_cache: Dict[str, AnomalyResult] = {}
        
        # æ€§èƒ½ç»„ä»¶
        self.executor = ThreadPoolExecutor(max_workers=self.config.max_workers)
        self.scaler = StandardScaler() if SKLEARN_AVAILABLE else None
        
        # åˆå§‹åŒ–é»˜è®¤æ¨¡å¼å’Œé˜ˆå€¼
        self._initialize_default_patterns()
        self._initialize_default_thresholds()
        
        self.logger.info("å¼‚å¸¸æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_default_patterns(self):
        """åˆå§‹åŒ–é»˜è®¤å¼‚å¸¸æ¨¡å¼"""
        default_patterns = [
            # é¢„ç®—å¼‚å¸¸æ¨¡å¼
            AnomalyPattern(
                pattern_id="budget_spike",
                pattern_name="é¢„ç®—å¼‚å¸¸å¢é•¿",
                pattern_type=AnomalyType.STATISTICAL,
                description="é¡¹ç›®é¢„ç®—å‡ºç°å¼‚å¸¸å¢é•¿",
                indicators=["budget_amount", "budget_change_rate"],
                threshold_config={"z_score": 3.0, "percentage_change": 0.5},
                detection_method=DetectionMethod.Z_SCORE,
                severity_mapping={"high": AnomalySeverity.HIGH, "medium": AnomalySeverity.MEDIUM}
            ),
            
            # æ—¶é—´å¼‚å¸¸æ¨¡å¼
            AnomalyPattern(
                pattern_id="timeline_deviation",
                pattern_name="æ—¶é—´çº¿åå·®",
                pattern_type=AnomalyType.TEMPORAL,
                description="é¡¹ç›®æ—¶é—´çº¿å‡ºç°å¼‚å¸¸åå·®",
                indicators=["planned_duration", "actual_duration", "delay_days"],
                threshold_config={"delay_threshold": 30, "acceleration_threshold": -15},
                detection_method=DetectionMethod.CUSTOM_RULE,
                severity_mapping={"critical": AnomalySeverity.CRITICAL, "high": AnomalySeverity.HIGH}
            ),
            
            # ä¾›åº”å•†è¡Œä¸ºå¼‚å¸¸
            AnomalyPattern(
                pattern_id="supplier_behavior",
                pattern_name="ä¾›åº”å•†è¡Œä¸ºå¼‚å¸¸",
                pattern_type=AnomalyType.BEHAVIORAL,
                description="ä¾›åº”å•†æŠ•æ ‡æˆ–å±¥çº¦è¡Œä¸ºå¼‚å¸¸",
                indicators=["bid_frequency", "win_rate", "performance_score"],
                threshold_config={"frequency_threshold": 10, "win_rate_threshold": 0.8},
                detection_method=DetectionMethod.ISOLATION_FOREST,
                severity_mapping={"high": AnomalySeverity.HIGH, "medium": AnomalySeverity.MEDIUM}
            ),
            
            # ä»·æ ¼å¼‚å¸¸æ¨¡å¼
            AnomalyPattern(
                pattern_id="price_anomaly",
                pattern_name="ä»·æ ¼å¼‚å¸¸",
                pattern_type=AnomalyType.OUTLIER,
                description="æŠ•æ ‡ä»·æ ¼æˆ–é‡‡è´­ä»·æ ¼å¼‚å¸¸",
                indicators=["bid_price", "market_price", "price_deviation"],
                threshold_config={"deviation_threshold": 0.3, "outlier_factor": 2.0},
                detection_method=DetectionMethod.IQR,
                severity_mapping={"critical": AnomalySeverity.CRITICAL, "high": AnomalySeverity.HIGH}
            )
        ]
        
        for pattern in default_patterns:
            self.patterns[pattern.pattern_id] = pattern
    
    def _initialize_default_thresholds(self):
        """åˆå§‹åŒ–é»˜è®¤é˜ˆå€¼"""
        default_thresholds = [
            AnomalyThreshold(
                field_name="budget_amount",
                threshold_type="upper",
                threshold_value=10000000,  # 1000ä¸‡
                severity=AnomalySeverity.HIGH,
                description="é¢„ç®—é‡‘é¢è¶…è¿‡1000ä¸‡éœ€è¦ç‰¹åˆ«å…³æ³¨"
            ),
            AnomalyThreshold(
                field_name="delay_days",
                threshold_type="upper",
                threshold_value=60,
                severity=AnomalySeverity.CRITICAL,
                description="é¡¹ç›®å»¶æœŸè¶…è¿‡60å¤©ä¸ºä¸¥é‡å¼‚å¸¸"
            ),
            AnomalyThreshold(
                field_name="supplier_performance",
                threshold_type="lower",
                threshold_value=0.6,
                severity=AnomalySeverity.HIGH,
                description="ä¾›åº”å•†ç»©æ•ˆä½äº60%éœ€è¦å…³æ³¨"
            )
        ]
        
        for threshold in default_thresholds:
            self.thresholds[threshold.field_name] = threshold
    
    async def detect_anomalies(
        self,
        data: Union[Dict[str, Any], pd.DataFrame],
        project_id: str,
        detection_types: Optional[List[AnomalyType]] = None
    ) -> AnomalyReport:
        """
        æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
        
        Args:
            data: è¦æ£€æµ‹çš„æ•°æ®
            project_id: é¡¹ç›®ID
            detection_types: è¦æ‰§è¡Œçš„æ£€æµ‹ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™æ‰§è¡Œæ‰€æœ‰ç±»å‹
            
        Returns:
            AnomalyReport: å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
        """
        start_time = datetime.now()
        
        try:
            # æ•°æ®é¢„å¤„ç†
            processed_data = self._preprocess_data(data)
            
            # æ‰§è¡Œä¸åŒç±»å‹çš„å¼‚å¸¸æ£€æµ‹
            all_anomalies = []
            
            if not detection_types:
                detection_types = list(AnomalyType)
            
            # å¹¶è¡Œæ‰§è¡Œä¸åŒç±»å‹çš„æ£€æµ‹
            detection_tasks = []
            
            for anomaly_type in detection_types:
                task = self._detect_by_type(processed_data, anomaly_type, project_id)
                detection_tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰æ£€æµ‹ä»»åŠ¡å®Œæˆ
            detection_results = await asyncio.gather(*detection_tasks, return_exceptions=True)
            
            # æ”¶é›†ç»“æœ
            for result in detection_results:
                if isinstance(result, Exception):
                    self.logger.error(f"æ£€æµ‹ä»»åŠ¡å¼‚å¸¸: {str(result)}")
                elif isinstance(result, list):
                    all_anomalies.extend(result)
            
            # å»é‡å’Œæ’åº
            unique_anomalies = self._deduplicate_anomalies(all_anomalies)
            sorted_anomalies = sorted(
                unique_anomalies,
                key=lambda x: (x.severity.value, -x.confidence_score)
            )
            
            # é™åˆ¶å¼‚å¸¸æ•°é‡
            if len(sorted_anomalies) > self.config.max_anomalies_per_check:
                sorted_anomalies = sorted_anomalies[:self.config.max_anomalies_per_check]
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(
                project_id, sorted_anomalies, start_time, processed_data
            )
            
            # æ›´æ–°å†å²è®°å½•
            self.detection_history.append(report)
            
            # æ›´æ–°è¡Œä¸ºç”»åƒ
            await self._update_behavior_profiles(project_id, sorted_anomalies, processed_data)
            
            self.logger.info(
                f"å¼‚å¸¸æ£€æµ‹å®Œæˆ: é¡¹ç›®{project_id}, å‘ç°{len(sorted_anomalies)}ä¸ªå¼‚å¸¸, "
                f"è€—æ—¶{report.detection_duration.total_seconds():.2f}ç§’"
            )
            
            return report
            
        except Exception as e:
            self.logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
            raise
    
    def _preprocess_data(self, data: Union[Dict[str, Any], pd.DataFrame]) -> pd.DataFrame:
        """
        æ•°æ®é¢„å¤„ç†
        
        Args:
            data: åŸå§‹æ•°æ®
            
        Returns:
            pd.DataFrame: é¢„å¤„ç†åçš„æ•°æ®
        """
        if isinstance(data, dict):
            # å°†å­—å…¸è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame([data])
        elif isinstance(data, pd.DataFrame):
            df = data.copy()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(data)}")
        
        # æ•°æ®æ¸…æ´—
        # å¤„ç†ç¼ºå¤±å€¼
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())
        
        # å¤„ç†æ— ç©·å¤§å€¼
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna(0)
        
        # æ•°æ®ç±»å‹è½¬æ¢
        for col in df.columns:
            if df[col].dtype == 'object':
                try:
                    df[col] = pd.to_numeric(df[col], errors='ignore')
                except:
                    pass
        
        return df
    
    async def _detect_by_type(
        self,
        data: pd.DataFrame,
        anomaly_type: AnomalyType,
        project_id: str
    ) -> List[AnomalyResult]:
        """
        æŒ‰ç±»å‹æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
        
        Args:
            data: é¢„å¤„ç†åçš„æ•°æ®
            anomaly_type: å¼‚å¸¸ç±»å‹
            project_id: é¡¹ç›®ID
            
        Returns:
            List[AnomalyResult]: æ£€æµ‹åˆ°çš„å¼‚å¸¸åˆ—è¡¨
        """
        anomalies = []
        
        try:
            if anomaly_type == AnomalyType.STATISTICAL:
                anomalies.extend(await self._detect_statistical_anomalies(data, project_id))
            
            elif anomaly_type == AnomalyType.BEHAVIORAL:
                anomalies.extend(await self._detect_behavioral_anomalies(data, project_id))
            
            elif anomaly_type == AnomalyType.TEMPORAL:
                anomalies.extend(await self._detect_temporal_anomalies(data, project_id))
            
            elif anomaly_type == AnomalyType.PATTERN:
                anomalies.extend(await self._detect_pattern_anomalies(data, project_id))
            
            elif anomaly_type == AnomalyType.OUTLIER:
                anomalies.extend(await self._detect_outliers(data, project_id))
            
            elif anomaly_type == AnomalyType.THRESHOLD:
                anomalies.extend(await self._detect_threshold_anomalies(data, project_id))
            
        except Exception as e:
            self.logger.error(f"æ£€æµ‹ç±»å‹{anomaly_type.value}æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        
        return anomalies
    
    async def _detect_statistical_anomalies(
        self,
        data: pd.DataFrame,
        project_id: str
    ) -> List[AnomalyResult]:
        """
        ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
        
        ä½¿ç”¨Zåˆ†æ•°ã€IQRç­‰ç»Ÿè®¡æ–¹æ³•æ£€æµ‹å¼‚å¸¸
        """
        anomalies = []
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        
        for column in numeric_columns:
            values = data[column].dropna()
            if len(values) < 2:
                continue
            
            # Zåˆ†æ•°æ£€æµ‹
            z_scores = np.abs((values - values.mean()) / values.std())
            z_anomalies = values[z_scores > self.config.z_score_threshold]
            
            for idx, value in z_anomalies.items():
                anomaly = AnomalyResult(
                    anomaly_id=f"stat_z_{project_id}_{column}_{idx}",
                    anomaly_type=AnomalyType.STATISTICAL,
                    severity=self._calculate_severity_from_z_score(z_scores.iloc[idx]),
                    confidence_score=min(z_scores.iloc[idx] / 5.0, 1.0),
                    anomaly_score=z_scores.iloc[idx],
                    title=f"{column}å­—æ®µZåˆ†æ•°å¼‚å¸¸",
                    description=f"{column}çš„å€¼{value}çš„Zåˆ†æ•°ä¸º{z_scores.iloc[idx]:.2f}ï¼Œè¶…è¿‡é˜ˆå€¼{self.config.z_score_threshold}",
                    affected_fields=[column],
                    detection_method=DetectionMethod.Z_SCORE,
                    detection_time=datetime.now(),
                    original_value=value,
                    expected_value=values.mean(),
                    deviation=abs(value - values.mean()),
                    threshold_value=self.config.z_score_threshold
                )
                anomalies.append(anomaly)
            
            # IQRæ£€æµ‹
            Q1 = values.quantile(0.25)
            Q3 = values.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - self.config.iqr_multiplier * IQR
            upper_bound = Q3 + self.config.iqr_multiplier * IQR
            
            iqr_anomalies = values[(values < lower_bound) | (values > upper_bound)]
            
            for idx, value in iqr_anomalies.items():
                anomaly = AnomalyResult(
                    anomaly_id=f"stat_iqr_{project_id}_{column}_{idx}",
                    anomaly_type=AnomalyType.STATISTICAL,
                    severity=AnomalySeverity.MEDIUM,
                    confidence_score=0.8,
                    anomaly_score=abs(value - values.median()) / IQR if IQR > 0 else 0,
                    title=f"{column}å­—æ®µIQRå¼‚å¸¸",
                    description=f"{column}çš„å€¼{value}è¶…å‡ºIQRèŒƒå›´[{lower_bound:.2f}, {upper_bound:.2f}]",
                    affected_fields=[column],
                    detection_method=DetectionMethod.IQR,
                    detection_time=datetime.now(),
                    original_value=value,
                    expected_value=values.median(),
                    deviation=abs(value - values.median()),
                    threshold_value=(lower_bound, upper_bound)
                )
                anomalies.append(anomaly)
        
        return anomalies
    
    async def _detect_behavioral_anomalies(
        self,
        data: pd.DataFrame,
        project_id: str
    ) -> List[AnomalyResult]:
        """
        è¡Œä¸ºå¼‚å¸¸æ£€æµ‹
        
        æ£€æµ‹ä¾›åº”å•†ã€é‡‡è´­æ–¹ç­‰å®ä½“çš„å¼‚å¸¸è¡Œä¸ºæ¨¡å¼
        """
        anomalies = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¾›åº”å•†ç›¸å…³å­—æ®µ
        supplier_fields = [col for col in data.columns if 'supplier' in col.lower()]
        
        if supplier_fields:
            # ä¾›åº”å•†è¡Œä¸ºåˆ†æ
            for _, row in data.iterrows():
                supplier_id = row.get('supplier_id', 'unknown')
                
                # è·å–æˆ–åˆ›å»ºè¡Œä¸ºç”»åƒ
                profile = self.behavior_profiles.get(
                    supplier_id,
                    BehaviorProfile(
                        entity_id=supplier_id,
                        entity_type='supplier',
                        profile_period=(datetime.now() - timedelta(days=90), datetime.now())
                    )
                )
                
                # æ£€æŸ¥æŠ•æ ‡é¢‘ç‡å¼‚å¸¸
                bid_frequency = row.get('bid_frequency', 0)
                if bid_frequency > 20:  # å¼‚å¸¸é«˜é¢‘æŠ•æ ‡
                    anomaly = AnomalyResult(
                        anomaly_id=f"behavior_freq_{project_id}_{supplier_id}",
                        anomaly_type=AnomalyType.BEHAVIORAL,
                        severity=AnomalySeverity.HIGH,
                        confidence_score=0.85,
                        anomaly_score=bid_frequency / 20.0,
                        title="ä¾›åº”å•†å¼‚å¸¸é«˜é¢‘æŠ•æ ‡",
                        description=f"ä¾›åº”å•†{supplier_id}æŠ•æ ‡é¢‘ç‡{bid_frequency}æ¬¡å¼‚å¸¸åé«˜",
                        affected_fields=['bid_frequency'],
                        detection_method=DetectionMethod.CUSTOM_RULE,
                        detection_time=datetime.now(),
                        original_value=bid_frequency,
                        expected_value=10,
                        context_data={'supplier_id': supplier_id}
                    )
                    anomalies.append(anomaly)
                
                # æ£€æŸ¥ä¸­æ ‡ç‡å¼‚å¸¸
                win_rate = row.get('win_rate', 0)
                if win_rate > 0.8:  # å¼‚å¸¸é«˜ä¸­æ ‡ç‡
                    anomaly = AnomalyResult(
                        anomaly_id=f"behavior_winrate_{project_id}_{supplier_id}",
                        anomaly_type=AnomalyType.BEHAVIORAL,
                        severity=AnomalySeverity.HIGH,
                        confidence_score=0.9,
                        anomaly_score=win_rate,
                        title="ä¾›åº”å•†å¼‚å¸¸é«˜ä¸­æ ‡ç‡",
                        description=f"ä¾›åº”å•†{supplier_id}ä¸­æ ‡ç‡{win_rate:.1%}å¼‚å¸¸åé«˜ï¼Œå¯èƒ½å­˜åœ¨ä¸å½“ç«äº‰",
                        affected_fields=['win_rate'],
                        detection_method=DetectionMethod.CUSTOM_RULE,
                        detection_time=datetime.now(),
                        original_value=win_rate,
                        expected_value=0.3,
                        context_data={'supplier_id': supplier_id}
                    )
                    anomalies.append(anomaly)
        
        return anomalies
    
    async def _detect_temporal_anomalies(
        self,
        data: pd.DataFrame,
        project_id: str
    ) -> List[AnomalyResult]:
        """
        æ—¶é—´å¼‚å¸¸æ£€æµ‹
        
        æ£€æµ‹é¡¹ç›®æ—¶é—´çº¿ã€å­£èŠ‚æ€§ç­‰æ—¶é—´ç›¸å…³å¼‚å¸¸
        """
        anomalies = []
        
        # æ£€æŸ¥æ—¶é—´ç›¸å…³å­—æ®µ
        time_fields = [col for col in data.columns if any(keyword in col.lower() 
                      for keyword in ['time', 'date', 'duration', 'delay'])]
        
        for _, row in data.iterrows():
            # æ£€æŸ¥é¡¹ç›®å»¶æœŸ
            if 'delay_days' in row:
                delay_days = row['delay_days']
                if delay_days > 30:  # å»¶æœŸè¶…è¿‡30å¤©
                    severity = AnomalySeverity.CRITICAL if delay_days > 90 else AnomalySeverity.HIGH
                    anomaly = AnomalyResult(
                        anomaly_id=f"temporal_delay_{project_id}",
                        anomaly_type=AnomalyType.TEMPORAL,
                        severity=severity,
                        confidence_score=0.95,
                        anomaly_score=delay_days / 30.0,
                        title="é¡¹ç›®ä¸¥é‡å»¶æœŸ",
                        description=f"é¡¹ç›®å»¶æœŸ{delay_days}å¤©ï¼Œè¶…å‡ºæ­£å¸¸èŒƒå›´",
                        affected_fields=['delay_days'],
                        detection_method=DetectionMethod.CUSTOM_RULE,
                        detection_time=datetime.now(),
                        original_value=delay_days,
                        expected_value=0,
                        threshold_value=30
                    )
                    anomalies.append(anomaly)
            
            # æ£€æŸ¥æ‰§è¡Œæ—¶é—´å¼‚å¸¸
            if 'planned_duration' in row and 'actual_duration' in row:
                planned = row['planned_duration']
                actual = row['actual_duration']
                if planned > 0:
                    deviation_ratio = abs(actual - planned) / planned
                    if deviation_ratio > 0.5:  # åå·®è¶…è¿‡50%
                        anomaly = AnomalyResult(
                            anomaly_id=f"temporal_duration_{project_id}",
                            anomaly_type=AnomalyType.TEMPORAL,
                            severity=AnomalySeverity.MEDIUM,
                            confidence_score=0.8,
                            anomaly_score=deviation_ratio,
                            title="é¡¹ç›®æ‰§è¡Œæ—¶é—´å¼‚å¸¸",
                            description=f"å®é™…æ‰§è¡Œæ—¶é—´{actual}ä¸è®¡åˆ’æ—¶é—´{planned}åå·®{deviation_ratio:.1%}",
                            affected_fields=['planned_duration', 'actual_duration'],
                            detection_method=DetectionMethod.CUSTOM_RULE,
                            detection_time=datetime.now(),
                            original_value=actual,
                            expected_value=planned,
                            deviation=abs(actual - planned)
                        )
                        anomalies.append(anomaly)
        
        return anomalies
    
    async def _detect_pattern_anomalies(
        self,
        data: pd.DataFrame,
        project_id: str
    ) -> List[AnomalyResult]:
        """
        æ¨¡å¼å¼‚å¸¸æ£€æµ‹
        
        ä½¿ç”¨é¢„å®šä¹‰çš„å¼‚å¸¸æ¨¡å¼è¿›è¡Œæ£€æµ‹
        """
        anomalies = []
        
        for pattern_id, pattern in self.patterns.items():
            if not pattern.enabled:
                continue
            
            try:
                # æ£€æŸ¥æ¨¡å¼æ‰€éœ€çš„æŒ‡æ ‡æ˜¯å¦å­˜åœ¨
                available_indicators = [ind for ind in pattern.indicators if ind in data.columns]
                if len(available_indicators) < len(pattern.indicators) * 0.5:  # è‡³å°‘50%çš„æŒ‡æ ‡å¯ç”¨
                    continue
                
                # æ ¹æ®æ£€æµ‹æ–¹æ³•æ‰§è¡Œæ¨¡å¼æ£€æµ‹
                pattern_anomalies = await self._apply_pattern_detection(
                    data, pattern, project_id, available_indicators
                )
                anomalies.extend(pattern_anomalies)
                
            except Exception as e:
                self.logger.error(f"æ¨¡å¼{pattern_id}æ£€æµ‹å¤±è´¥: {str(e)}")
        
        return anomalies
    
    async def _apply_pattern_detection(
        self,
        data: pd.DataFrame,
        pattern: AnomalyPattern,
        project_id: str,
        indicators: List[str]
    ) -> List[AnomalyResult]:
        """
        åº”ç”¨ç‰¹å®šæ¨¡å¼è¿›è¡Œæ£€æµ‹
        """
        anomalies = []
        
        if pattern.detection_method == DetectionMethod.Z_SCORE:
            # ä½¿ç”¨Zåˆ†æ•°æ–¹æ³•
            for indicator in indicators:
                if indicator in data.columns:
                    values = data[indicator].dropna()
                    if len(values) > 1:
                        z_scores = np.abs((values - values.mean()) / values.std())
                        threshold = pattern.threshold_config.get('z_score', 3.0)
                        
                        anomalous_indices = z_scores[z_scores > threshold].index
                        for idx in anomalous_indices:
                            anomaly = AnomalyResult(
                                anomaly_id=f"pattern_{pattern.pattern_id}_{project_id}_{indicator}_{idx}",
                                anomaly_type=pattern.pattern_type,
                                severity=AnomalySeverity.MEDIUM,
                                confidence_score=min(z_scores.iloc[idx] / 5.0, 1.0),
                                anomaly_score=z_scores.iloc[idx],
                                title=f"{pattern.pattern_name} - {indicator}",
                                description=f"åœ¨{indicator}å­—æ®µæ£€æµ‹åˆ°{pattern.description}",
                                affected_fields=[indicator],
                                detection_method=pattern.detection_method,
                                detection_time=datetime.now(),
                                original_value=values.iloc[idx],
                                expected_value=values.mean()
                            )
                            anomalies.append(anomaly)
        
        elif pattern.detection_method == DetectionMethod.CUSTOM_RULE:
            # è‡ªå®šä¹‰è§„åˆ™æ£€æµ‹
            anomalies.extend(await self._apply_custom_rule_detection(
                data, pattern, project_id, indicators
            ))
        
        return anomalies
    
    async def _apply_custom_rule_detection(
        self,
        data: pd.DataFrame,
        pattern: AnomalyPattern,
        project_id: str,
        indicators: List[str]
    ) -> List[AnomalyResult]:
        """
        åº”ç”¨è‡ªå®šä¹‰è§„åˆ™æ£€æµ‹
        """
        anomalies = []
        
        # æ ¹æ®æ¨¡å¼IDåº”ç”¨ç‰¹å®šè§„åˆ™
        if pattern.pattern_id == "budget_spike":
            # é¢„ç®—å¼‚å¸¸å¢é•¿æ£€æµ‹
            if 'budget_change_rate' in data.columns:
                for idx, row in data.iterrows():
                    change_rate = row.get('budget_change_rate', 0)
                    threshold = pattern.threshold_config.get('percentage_change', 0.5)
                    
                    if abs(change_rate) > threshold:
                        severity = AnomalySeverity.HIGH if abs(change_rate) > 1.0 else AnomalySeverity.MEDIUM
                        anomaly = AnomalyResult(
                            anomaly_id=f"budget_spike_{project_id}_{idx}",
                            anomaly_type=AnomalyType.PATTERN,
                            severity=severity,
                            confidence_score=0.9,
                            anomaly_score=abs(change_rate),
                            title="é¢„ç®—å¼‚å¸¸å˜åŒ–",
                            description=f"é¢„ç®—å˜åŒ–ç‡{change_rate:.1%}è¶…è¿‡é˜ˆå€¼{threshold:.1%}",
                            affected_fields=['budget_change_rate'],
                            detection_method=DetectionMethod.CUSTOM_RULE,
                            detection_time=datetime.now(),
                            original_value=change_rate,
                            threshold_value=threshold
                        )
                        anomalies.append(anomaly)
        
        elif pattern.pattern_id == "timeline_deviation":
            # æ—¶é—´çº¿åå·®æ£€æµ‹
            if 'delay_days' in data.columns:
                for idx, row in data.iterrows():
                    delay_days = row.get('delay_days', 0)
                    delay_threshold = pattern.threshold_config.get('delay_threshold', 30)
                    acceleration_threshold = pattern.threshold_config.get('acceleration_threshold', -15)
                    
                    if delay_days > delay_threshold:
                        anomaly = AnomalyResult(
                            anomaly_id=f"timeline_delay_{project_id}_{idx}",
                            anomaly_type=AnomalyType.PATTERN,
                            severity=AnomalySeverity.CRITICAL if delay_days > 60 else AnomalySeverity.HIGH,
                            confidence_score=0.95,
                            anomaly_score=delay_days / delay_threshold,
                            title="é¡¹ç›®ä¸¥é‡å»¶æœŸ",
                            description=f"é¡¹ç›®å»¶æœŸ{delay_days}å¤©ï¼Œè¶…è¿‡é˜ˆå€¼{delay_threshold}å¤©",
                            affected_fields=['delay_days'],
                            detection_method=DetectionMethod.CUSTOM_RULE,
                            detection_time=datetime.now(),
                            original_value=delay_days,
                            threshold_value=delay_threshold
                        )
                        anomalies.append(anomaly)
                    
                    elif delay_days < acceleration_threshold:
                        anomaly = AnomalyResult(
                            anomaly_id=f"timeline_acceleration_{project_id}_{idx}",
                            anomaly_type=AnomalyType.PATTERN,
                            severity=AnomalySeverity.MEDIUM,
                            confidence_score=0.8,
                            anomaly_score=abs(delay_days / acceleration_threshold),
                            title="é¡¹ç›®å¼‚å¸¸æå‰",
                            description=f"é¡¹ç›®æå‰{abs(delay_days)}å¤©å®Œæˆï¼Œå¯èƒ½å­˜åœ¨è´¨é‡é£é™©",
                            affected_fields=['delay_days'],
                            detection_method=DetectionMethod.CUSTOM_RULE,
                            detection_time=datetime.now(),
                            original_value=delay_days,
                            threshold_value=acceleration_threshold
                        )
                        anomalies.append(anomaly)
        
        return anomalies
    
    async def _detect_outliers(
        self,
        data: pd.DataFrame,
        project_id: str
    ) -> List[AnomalyResult]:
        """
        ç¦»ç¾¤å€¼æ£€æµ‹
        
        ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•æ£€æµ‹ç¦»ç¾¤å€¼
        """
        anomalies = []
        
        if not SKLEARN_AVAILABLE:
            self.logger.warning("scikit-learnä¸å¯ç”¨ï¼Œè·³è¿‡æœºå™¨å­¦ä¹ ç¦»ç¾¤å€¼æ£€æµ‹")
            return anomalies
        
        # è·å–æ•°å€¼åˆ—
        numeric_data = data.select_dtypes(include=[np.number])
        if numeric_data.empty or len(numeric_data) < 2:
            return anomalies
        
        try:
            # æ•°æ®æ ‡å‡†åŒ–
            scaled_data = self.scaler.fit_transform(numeric_data.fillna(0))
            
            # å­¤ç«‹æ£®æ—æ£€æµ‹
            iso_forest = IsolationForest(
                contamination=self.config.contamination_rate,
                random_state=self.config.random_state,
                n_estimators=self.config.n_estimators
            )
            outlier_labels = iso_forest.fit_predict(scaled_data)
            outlier_scores = iso_forest.score_samples(scaled_data)
            
            # ç”Ÿæˆå¼‚å¸¸ç»“æœ
            for idx, (label, score) in enumerate(zip(outlier_labels, outlier_scores)):
                if label == -1:  # ç¦»ç¾¤å€¼
                    anomaly = AnomalyResult(
                        anomaly_id=f"outlier_isolation_{project_id}_{idx}",
                        anomaly_type=AnomalyType.OUTLIER,
                        severity=self._calculate_severity_from_score(-score),
                        confidence_score=min(abs(score) * 2, 1.0),
                        anomaly_score=-score,
                        title="æ•°æ®ç¦»ç¾¤å€¼",
                        description=f"ç¬¬{idx}è¡Œæ•°æ®è¢«è¯†åˆ«ä¸ºç¦»ç¾¤å€¼ï¼Œå¼‚å¸¸åˆ†æ•°{-score:.3f}",
                        affected_fields=list(numeric_data.columns),
                        detection_method=DetectionMethod.ISOLATION_FOREST,
                        detection_time=datetime.now(),
                        metadata={'isolation_score': score, 'row_index': idx}
                    )
                    anomalies.append(anomaly)
            
            # å±€éƒ¨ç¦»ç¾¤å› å­æ£€æµ‹
            if len(scaled_data) >= 20:  # LOFéœ€è¦è¶³å¤Ÿçš„æ ·æœ¬
                lof = LocalOutlierFactor(
                    n_neighbors=min(20, len(scaled_data) - 1),
                    contamination=self.config.contamination_rate
                )
                lof_labels = lof.fit_predict(scaled_data)
                lof_scores = lof.negative_outlier_factor_
                
                for idx, (label, score) in enumerate(zip(lof_labels, lof_scores)):
                    if label == -1:  # ç¦»ç¾¤å€¼
                        anomaly = AnomalyResult(
                            anomaly_id=f"outlier_lof_{project_id}_{idx}",
                            anomaly_type=AnomalyType.OUTLIER,
                            severity=self._calculate_severity_from_score(-score),
                            confidence_score=min(abs(score), 1.0),
                            anomaly_score=-score,
                            title="å±€éƒ¨ç¦»ç¾¤å€¼",
                            description=f"ç¬¬{idx}è¡Œæ•°æ®è¢«è¯†åˆ«ä¸ºå±€éƒ¨ç¦»ç¾¤å€¼ï¼ŒLOFåˆ†æ•°{score:.3f}",
                            affected_fields=list(numeric_data.columns),
                            detection_method=DetectionMethod.LOCAL_OUTLIER_FACTOR,
                            detection_time=datetime.now(),
                            metadata={'lof_score': score, 'row_index': idx}
                        )
                        anomalies.append(anomaly)
        
        except Exception as e:
            self.logger.error(f"æœºå™¨å­¦ä¹ ç¦»ç¾¤å€¼æ£€æµ‹å¤±è´¥: {str(e)}")
        
        return anomalies
    
    async def _detect_threshold_anomalies(
        self,
        data: pd.DataFrame,
        project_id: str
    ) -> List[AnomalyResult]:
        """
        é˜ˆå€¼å¼‚å¸¸æ£€æµ‹
        
        åŸºäºé¢„å®šä¹‰é˜ˆå€¼æ£€æµ‹å¼‚å¸¸
        """
        anomalies = []
        
        for field_name, threshold in self.thresholds.items():
            if not threshold.enabled or field_name not in data.columns:
                continue
            
            for idx, row in data.iterrows():
                value = row.get(field_name)
                if value is None:
                    continue
                
                if threshold.check_threshold(value):
                    anomaly = AnomalyResult(
                        anomaly_id=f"threshold_{field_name}_{project_id}_{idx}",
                        anomaly_type=AnomalyType.THRESHOLD,
                        severity=threshold.severity,
                        confidence_score=0.9,
                        anomaly_score=self._calculate_threshold_score(value, threshold),
                        title=f"{field_name}é˜ˆå€¼å¼‚å¸¸",
                        description=f"{field_name}çš„å€¼{value}è¶…è¿‡é˜ˆå€¼{threshold.threshold_value}ã€‚{threshold.description}",
                        affected_fields=[field_name],
                        detection_method=DetectionMethod.CUSTOM_RULE,
                        detection_time=datetime.now(),
                        original_value=value,
                        threshold_value=threshold.threshold_value
                    )
                    anomalies.append(anomaly)
        
        return anomalies
    
    def _calculate_severity_from_z_score(self, z_score: float) -> AnomalySeverity:
        """æ ¹æ®Zåˆ†æ•°è®¡ç®—ä¸¥é‡ç¨‹åº¦"""
        if z_score >= 5.0:
            return AnomalySeverity.CRITICAL
        elif z_score >= 4.0:
            return AnomalySeverity.HIGH
        elif z_score >= 3.0:
            return AnomalySeverity.MEDIUM
        else:
            return AnomalySeverity.LOW
    
    def _calculate_severity_from_score(self, score: float) -> AnomalySeverity:
        """æ ¹æ®å¼‚å¸¸åˆ†æ•°è®¡ç®—ä¸¥é‡ç¨‹åº¦"""
        if score >= 0.8:
            return AnomalySeverity.CRITICAL
        elif score >= 0.6:
            return AnomalySeverity.HIGH
        elif score >= 0.4:
            return AnomalySeverity.MEDIUM
        else:
            return AnomalySeverity.LOW
    
    def _calculate_threshold_score(self, value: float, threshold: AnomalyThreshold) -> float:
        """è®¡ç®—é˜ˆå€¼å¼‚å¸¸åˆ†æ•°"""
        if threshold.threshold_type == "upper":
            return max(0, (value - threshold.threshold_value) / threshold.threshold_value)
        elif threshold.threshold_type == "lower":
            return max(0, (threshold.threshold_value - value) / threshold.threshold_value)
        elif threshold.threshold_type == "range":
            min_val, max_val = threshold.threshold_value
            if value < min_val:
                return (min_val - value) / (max_val - min_val)
            elif value > max_val:
                return (value - max_val) / (max_val - min_val)
        return 0.0
    
    def _deduplicate_anomalies(self, anomalies: List[AnomalyResult]) -> List[AnomalyResult]:
        """å»é™¤é‡å¤å¼‚å¸¸"""
        seen = set()
        unique_anomalies = []
        
        for anomaly in anomalies:
            # åˆ›å»ºå”¯ä¸€æ ‡è¯†
            key = f"{anomaly.anomaly_type.value}_{','.join(anomaly.affected_fields)}_{anomaly.original_value}"
            if key not in seen:
                seen.add(key)
                unique_anomalies.append(anomaly)
        
        return unique_anomalies
    
    def _generate_report(
        self,
        project_id: str,
        anomalies: List[AnomalyResult],
        start_time: datetime,
        data: pd.DataFrame
    ) -> AnomalyReport:
        """ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š"""
        end_time = datetime.now()
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = AnomalyMetrics(
            total_anomalies=len(anomalies),
            critical_anomalies=sum(1 for a in anomalies if a.severity == AnomalySeverity.CRITICAL),
            high_anomalies=sum(1 for a in anomalies if a.severity == AnomalySeverity.HIGH),
            medium_anomalies=sum(1 for a in anomalies if a.severity == AnomalySeverity.MEDIUM),
            low_anomalies=sum(1 for a in anomalies if a.severity == AnomalySeverity.LOW),
            info_anomalies=sum(1 for a in anomalies if a.severity == AnomalySeverity.INFO),
            statistical_anomalies=sum(1 for a in anomalies if a.anomaly_type == AnomalyType.STATISTICAL),
            behavioral_anomalies=sum(1 for a in anomalies if a.anomaly_type == AnomalyType.BEHAVIORAL),
            temporal_anomalies=sum(1 for a in anomalies if a.anomaly_type == AnomalyType.TEMPORAL),
            pattern_anomalies=sum(1 for a in anomalies if a.anomaly_type == AnomalyType.PATTERN),
            outlier_anomalies=sum(1 for a in anomalies if a.anomaly_type == AnomalyType.OUTLIER),
            average_confidence=sum(a.confidence_score for a in anomalies) / len(anomalies) if anomalies else 0.0,
            average_anomaly_score=sum(a.anomaly_score for a in anomalies) / len(anomalies) if anomalies else 0.0,
            detection_time_seconds=(end_time - start_time).total_seconds(),
            data_points_analyzed=len(data)
        )
        
        # ç¡®å®šæ•´ä½“é£é™©ç­‰çº§
        if metrics.critical_anomalies > 0:
            overall_risk_level = "æé«˜é£é™©"
        elif metrics.high_anomalies > 0:
            overall_risk_level = "é«˜é£é™©"
        elif metrics.medium_anomalies > 0:
            overall_risk_level = "ä¸­ç­‰é£é™©"
        elif metrics.low_anomalies > 0:
            overall_risk_level = "ä½é£é™©"
        else:
            overall_risk_level = "æ­£å¸¸"
        
        # ç”Ÿæˆå…³é”®å‘ç°
        key_findings = self._generate_key_findings(anomalies, metrics)
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(anomalies, metrics)
        
        return AnomalyReport(
            report_id=f"anomaly_{start_time.strftime('%Y%m%d_%H%M%S')}_{project_id}",
            project_id=project_id,
            detection_time=start_time,
            detection_duration=end_time - start_time,
            anomalies=anomalies,
            metrics=metrics,
            detection_config=self.config,
            patterns_used=list(self.patterns.values()),
            data_source="project_data",
            data_time_range=(start_time, end_time),
            overall_risk_level=overall_risk_level,
            key_findings=key_findings,
            recommendations=recommendations,
            metadata={
                "detector_version": "1.0.0",
                "sklearn_available": SKLEARN_AVAILABLE,
                "patterns_count": len(self.patterns),
                "thresholds_count": len(self.thresholds)
            }
        )
    
    def _generate_key_findings(self, anomalies: List[AnomalyResult], metrics: AnomalyMetrics) -> List[str]:
        """ç”Ÿæˆå…³é”®å‘ç°"""
        findings = []
        
        if metrics.critical_anomalies > 0:
            findings.append(f"å‘ç°{metrics.critical_anomalies}ä¸ªä¸¥é‡å¼‚å¸¸ï¼Œéœ€è¦ç«‹å³å¤„ç†")
        
        if metrics.high_anomalies > 0:
            findings.append(f"å‘ç°{metrics.high_anomalies}ä¸ªé«˜é£é™©å¼‚å¸¸ï¼Œå»ºè®®ä¼˜å…ˆå…³æ³¨")
        
        # åˆ†æå¼‚å¸¸ç±»å‹åˆ†å¸ƒ
        type_counts = metrics.get_type_distribution()
        max_type = max(type_counts.items(), key=lambda x: x[1])
        if max_type[1] > 0:
            findings.append(f"ä¸»è¦å¼‚å¸¸ç±»å‹ä¸º{max_type[0]}å¼‚å¸¸ï¼Œå…±{max_type[1]}ä¸ª")
        
        # åˆ†æç½®ä¿¡åº¦
        if metrics.average_confidence > 0.8:
            findings.append(f"å¼‚å¸¸æ£€æµ‹å¹³å‡ç½®ä¿¡åº¦{metrics.average_confidence:.1%}ï¼Œç»“æœå¯ä¿¡åº¦è¾ƒé«˜")
        elif metrics.average_confidence < 0.6:
            findings.append(f"å¼‚å¸¸æ£€æµ‹å¹³å‡ç½®ä¿¡åº¦{metrics.average_confidence:.1%}ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯")
        
        return findings
    
    def _generate_recommendations(self, anomalies: List[AnomalyResult], metrics: AnomalyMetrics) -> List[str]:
        """ç”Ÿæˆå»ºè®®æªæ–½"""
        recommendations = []
        
        if metrics.critical_anomalies > 0:
            recommendations.append("ğŸš¨ ç«‹å³è°ƒæŸ¥å’Œå¤„ç†æ‰€æœ‰ä¸¥é‡å¼‚å¸¸ï¼Œæš‚åœç›¸å…³é¡¹ç›®è¿›å±•")
        
        if metrics.high_anomalies > 0:
            recommendations.append("âš ï¸ ä¼˜å…ˆå¤„ç†é«˜é£é™©å¼‚å¸¸ï¼Œåˆ¶å®šåº”å¯¹æªæ–½")
        
        if metrics.behavioral_anomalies > 0:
            recommendations.append("ğŸ‘¥ åŠ å¼ºå¯¹ç›¸å…³ä¾›åº”å•†å’Œå‚ä¸æ–¹çš„ç›‘ç£å’Œå®¡æŸ¥")
        
        if metrics.temporal_anomalies > 0:
            recommendations.append("â° é‡æ–°è¯„ä¼°é¡¹ç›®æ—¶é—´å®‰æ’ï¼Œè°ƒæ•´è¿›åº¦è®¡åˆ’")
        
        if metrics.outlier_anomalies > 0:
            recommendations.append("ğŸ“Š æ·±å…¥åˆ†ææ•°æ®å¼‚å¸¸åŸå› ï¼Œæ£€æŸ¥æ•°æ®è´¨é‡")
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "ğŸ“‹ å»ºç«‹å¼‚å¸¸ç›‘æ§æœºåˆ¶ï¼Œå®šæœŸè¿›è¡Œå¼‚å¸¸æ£€æµ‹",
            "ğŸ“š å®Œå–„å¼‚å¸¸å¤„ç†æµç¨‹å’Œåº”æ€¥é¢„æ¡ˆ",
            "ğŸ”„ æŒç»­ä¼˜åŒ–æ£€æµ‹ç®—æ³•å’Œé˜ˆå€¼è®¾ç½®"
        ])
        
        return recommendations
    
    async def _update_behavior_profiles(
        self,
        project_id: str,
        anomalies: List[AnomalyResult],
        data: pd.DataFrame
    ):
        """æ›´æ–°è¡Œä¸ºç”»åƒ"""
        try:
            # æå–å®ä½“ä¿¡æ¯
            entities = set()
            
            # ä»æ•°æ®ä¸­æå–ä¾›åº”å•†ID
            if 'supplier_id' in data.columns:
                entities.update(data['supplier_id'].dropna().unique())
            
            # ä»å¼‚å¸¸ä¸­æå–ç›¸å…³å®ä½“
            for anomaly in anomalies:
                if 'supplier_id' in anomaly.context_data:
                    entities.add(anomaly.context_data['supplier_id'])
            
            # æ›´æ–°æ¯ä¸ªå®ä½“çš„è¡Œä¸ºç”»åƒ
            for entity_id in entities:
                if entity_id not in self.behavior_profiles:
                    self.behavior_profiles[entity_id] = BehaviorProfile(
                        entity_id=str(entity_id),
                        entity_type='supplier',
                        profile_period=(datetime.now() - timedelta(days=90), datetime.now())
                    )
                
                profile = self.behavior_profiles[entity_id]
                
                # æ›´æ–°å¼‚å¸¸å†å²
                entity_anomalies = [
                    a for a in anomalies 
                    if a.context_data.get('supplier_id') == entity_id
                ]
                
                if entity_anomalies:
                    profile.update_risk_score(entity_anomalies)
                    profile.anomaly_history.extend([a.anomaly_id for a in entity_anomalies])
        
        except Exception as e:
            self.logger.error(f"æ›´æ–°è¡Œä¸ºç”»åƒå¤±è´¥: {str(e)}")
    
    def add_pattern(self, pattern: AnomalyPattern):
        """æ·»åŠ å¼‚å¸¸æ¨¡å¼"""
        self.patterns[pattern.pattern_id] = pattern
        self.logger.info(f"æ·»åŠ å¼‚å¸¸æ¨¡å¼: {pattern.pattern_name}")
    
    def add_threshold(self, threshold: AnomalyThreshold):
        """æ·»åŠ é˜ˆå€¼é…ç½®"""
        self.thresholds[threshold.field_name] = threshold
        self.logger.info(f"æ·»åŠ é˜ˆå€¼é…ç½®: {threshold.field_name}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ£€æµ‹å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_detections": len(self.detection_history),
            "total_patterns": len(self.patterns),
            "total_thresholds": len(self.thresholds),
            "behavior_profiles": len(self.behavior_profiles),
            "cache_size": len(self.anomaly_cache),
            "sklearn_available": SKLEARN_AVAILABLE,
            "average_detection_time": sum(
                report.detection_duration.total_seconds() 
                for report in self.detection_history
            ) / len(self.detection_history) if self.detection_history else 0
        }