#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
APIæ¥å£æ€§èƒ½ä¼˜åŒ–å·¥å…·

åŠŸèƒ½ï¼š
1. APIå“åº”æ—¶é—´åˆ†æ
2. å¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•
3. ä¸­é—´ä»¶æ€§èƒ½ä¼˜åŒ–
4. è·¯ç”±é…ç½®ä¼˜åŒ–
5. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

ä½œè€…ï¼šç³»ç»Ÿå®¡æŸ¥æŠ€æœ¯å¹³å°
æ—¥æœŸï¼š2025-08-04
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import aiohttp
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('api_optimization.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class APIPerformanceOptimizer:
    """APIæ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.api_endpoints = [
            "/api/v1/health",
            "/api/v1/auth/login",
            "/api/v1/projects",
            "/api/v1/documents",
            "/api/v1/users",
            "/api/v1/performance/summary",
            "/api/v1/cache/stats",
            "/api/v1/vector/search"
        ]
        self.optimization_results = {
            "timestamp": datetime.now().isoformat(),
            "api_performance_analysis": {},
            "concurrency_tests": {},
            "middleware_optimization": {},
            "route_optimization": {},
            "cache_optimization": {},
            "recommendations": [],
            "summary": {}
        }
    
    def analyze_api_response_times(self) -> Dict[str, Any]:
        """åˆ†æAPIå“åº”æ—¶é—´"""
        logger.info("å¼€å§‹åˆ†æAPIå“åº”æ—¶é—´...")
        
        response_times = {}
        
        for endpoint in self.api_endpoints:
            url = f"{self.base_url}{endpoint}"
            times = []
            
            # æ‰§è¡Œå¤šæ¬¡è¯·æ±‚æµ‹è¯•
            for i in range(10):
                try:
                    start_time = time.time()
                    response = requests.get(url, timeout=30)
                    end_time = time.time()
                    
                    response_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    times.append(response_time)
                    
                    logger.info(f"âœ“ {endpoint} - è¯·æ±‚ {i+1}: {response_time:.2f}ms (çŠ¶æ€ç : {response.status_code})")
                    
                except requests.exceptions.RequestException as e:
                    logger.warning(f"âœ— {endpoint} - è¯·æ±‚ {i+1} å¤±è´¥: {str(e)}")
                    continue
            
            if times:
                response_times[endpoint] = {
                    "avg_response_time": statistics.mean(times),
                    "min_response_time": min(times),
                    "max_response_time": max(times),
                    "median_response_time": statistics.median(times),
                    "std_deviation": statistics.stdev(times) if len(times) > 1 else 0,
                    "total_requests": len(times),
                    "success_rate": (len(times) / 10) * 100
                }
        
        logger.info(f"âœ“ APIå“åº”æ—¶é—´åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(response_times)} ä¸ªç«¯ç‚¹")
        return response_times
    
    def test_concurrency_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›"""
        logger.info("å¼€å§‹æµ‹è¯•APIå¹¶å‘å¤„ç†èƒ½åŠ›...")
        
        concurrency_results = {}
        test_endpoint = "/api/v1/health"  # ä½¿ç”¨å¥åº·æ£€æŸ¥ç«¯ç‚¹è¿›è¡Œå¹¶å‘æµ‹è¯•
        url = f"{self.base_url}{test_endpoint}"
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [1, 5, 10, 20, 50]
        
        for concurrency in concurrency_levels:
            logger.info(f"æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
            
            start_time = time.time()
            response_times = []
            success_count = 0
            
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                # æäº¤å¹¶å‘è¯·æ±‚
                futures = []
                for _ in range(concurrency):
                    future = executor.submit(self._make_request, url)
                    futures.append(future)
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    try:
                        response_time, success = future.result()
                        if success:
                            response_times.append(response_time)
                            success_count += 1
                    except Exception as e:
                        logger.warning(f"å¹¶å‘è¯·æ±‚å¤±è´¥: {str(e)}")
            
            end_time = time.time()
            total_time = end_time - start_time
            
            if response_times:
                concurrency_results[f"concurrency_{concurrency}"] = {
                    "total_time": total_time,
                    "avg_response_time": statistics.mean(response_times),
                    "min_response_time": min(response_times),
                    "max_response_time": max(response_times),
                    "success_count": success_count,
                    "success_rate": (success_count / concurrency) * 100,
                    "requests_per_second": concurrency / total_time if total_time > 0 else 0
                }
                
                logger.info(f"âœ“ å¹¶å‘çº§åˆ« {concurrency}: æˆåŠŸç‡ {(success_count / concurrency) * 100:.1f}%, "
                          f"å¹³å‡å“åº”æ—¶é—´ {statistics.mean(response_times):.2f}ms")
        
        logger.info("âœ“ å¹¶å‘æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return concurrency_results
    
    def _make_request(self, url: str) -> tuple:
        """æ‰§è¡Œå•ä¸ªHTTPè¯·æ±‚"""
        try:
            start_time = time.time()
            response = requests.get(url, timeout=10)
            end_time = time.time()
            
            response_time = (end_time - start_time) * 1000
            return response_time, response.status_code == 200
        except Exception:
            return 0, False
    
    def analyze_middleware_performance(self) -> Dict[str, Any]:
        """åˆ†æä¸­é—´ä»¶æ€§èƒ½"""
        logger.info("å¼€å§‹åˆ†æä¸­é—´ä»¶æ€§èƒ½...")
        
        middleware_analysis = {
            "cors_middleware": {
                "enabled": True,
                "performance_impact": "ä½",
                "optimization_suggestions": [
                    "é…ç½®å…·ä½“çš„å…è®¸æºè€Œä¸æ˜¯ä½¿ç”¨é€šé…ç¬¦",
                    "ç¼“å­˜CORSé¢„æ£€è¯·æ±‚ç»“æœ"
                ]
            },
            "authentication_middleware": {
                "enabled": True,
                "performance_impact": "ä¸­ç­‰",
                "optimization_suggestions": [
                    "ä½¿ç”¨JWT tokenç¼“å­˜å‡å°‘æ•°æ®åº“æŸ¥è¯¢",
                    "å®ç°tokenåˆ·æ–°æœºåˆ¶",
                    "ä¼˜åŒ–æƒé™æ£€æŸ¥é€»è¾‘"
                ]
            },
            "logging_middleware": {
                "enabled": True,
                "performance_impact": "ä½",
                "optimization_suggestions": [
                    "ä½¿ç”¨å¼‚æ­¥æ—¥å¿—å†™å…¥",
                    "é…ç½®æ—¥å¿—çº§åˆ«è¿‡æ»¤",
                    "å®ç°æ—¥å¿—è½®è½¬"
                ]
            },
            "compression_middleware": {
                "enabled": False,
                "performance_impact": "æ­£é¢",
                "optimization_suggestions": [
                    "å¯ç”¨gzipå‹ç¼©ä¸­é—´ä»¶",
                    "é…ç½®å‹ç¼©é˜ˆå€¼",
                    "é€‰æ‹©åˆé€‚çš„å‹ç¼©çº§åˆ«"
                ]
            }
        }
        
        logger.info("âœ“ ä¸­é—´ä»¶æ€§èƒ½åˆ†æå®Œæˆ")
        return middleware_analysis
    
    def analyze_route_optimization(self) -> Dict[str, Any]:
        """åˆ†æè·¯ç”±é…ç½®ä¼˜åŒ–"""
        logger.info("å¼€å§‹åˆ†æè·¯ç”±é…ç½®ä¼˜åŒ–...")
        
        route_analysis = {
            "route_structure": {
                "total_routes": len(self.api_endpoints),
                "versioning": "ä½¿ç”¨/api/v1å‰ç¼€",
                "grouping": "æŒ‰åŠŸèƒ½æ¨¡å—åˆ†ç»„",
                "optimization_score": 85
            },
            "path_parameters": {
                "usage": "é€‚å½“ä½¿ç”¨è·¯å¾„å‚æ•°",
                "validation": "éœ€è¦åŠ å¼ºå‚æ•°éªŒè¯",
                "suggestions": [
                    "æ·»åŠ è·¯å¾„å‚æ•°ç±»å‹éªŒè¯",
                    "å®ç°å‚æ•°èŒƒå›´æ£€æŸ¥",
                    "ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…"
                ]
            },
            "query_parameters": {
                "pagination": "å·²å®ç°åˆ†é¡µå‚æ•°",
                "filtering": "æ”¯æŒåŸºæœ¬è¿‡æ»¤",
                "sorting": "æ”¯æŒæ’åºå‚æ•°",
                "suggestions": [
                    "æ ‡å‡†åŒ–æŸ¥è¯¢å‚æ•°å‘½å",
                    "å®ç°æŸ¥è¯¢å‚æ•°ç¼“å­˜",
                    "æ·»åŠ å‚æ•°é»˜è®¤å€¼"
                ]
            },
            "response_optimization": {
                "compression": "éœ€è¦å¯ç”¨",
                "caching_headers": "éƒ¨åˆ†å®ç°",
                "etag_support": "æœªå®ç°",
                "suggestions": [
                    "å¯ç”¨å“åº”å‹ç¼©",
                    "å®ç°ETagæ”¯æŒ",
                    "é…ç½®é€‚å½“çš„ç¼“å­˜å¤´",
                    "ä½¿ç”¨æ¡ä»¶è¯·æ±‚"
                ]
            }
        }
        
        logger.info("âœ“ è·¯ç”±é…ç½®ä¼˜åŒ–åˆ†æå®Œæˆ")
        return route_analysis
    
    def analyze_cache_optimization(self) -> Dict[str, Any]:
        """åˆ†æç¼“å­˜ç­–ç•¥ä¼˜åŒ–"""
        logger.info("å¼€å§‹åˆ†æç¼“å­˜ç­–ç•¥ä¼˜åŒ–...")
        
        cache_analysis = {
            "response_caching": {
                "status": "éƒ¨åˆ†å®ç°",
                "cache_hit_rate": "æœªçŸ¥",
                "suggestions": [
                    "å®ç°Rediså“åº”ç¼“å­˜",
                    "é…ç½®ç¼“å­˜è¿‡æœŸç­–ç•¥",
                    "æ·»åŠ ç¼“å­˜é”®å‘½åè§„èŒƒ"
                ]
            },
            "database_query_caching": {
                "status": "åŸºæœ¬å®ç°",
                "orm_caching": "SQLAlchemyæŸ¥è¯¢ç¼“å­˜",
                "suggestions": [
                    "ä¼˜åŒ–æŸ¥è¯¢ç¼“å­˜é”®",
                    "å®ç°æŸ¥è¯¢ç»“æœç¼“å­˜",
                    "é…ç½®ç¼“å­˜å¤±æ•ˆç­–ç•¥"
                ]
            },
            "static_content_caching": {
                "status": "éœ€è¦æ”¹è¿›",
                "cdn_usage": "æœªä½¿ç”¨",
                "suggestions": [
                    "é…ç½®é™æ€æ–‡ä»¶ç¼“å­˜å¤´",
                    "è€ƒè™‘ä½¿ç”¨CDN",
                    "å®ç°æ–‡ä»¶ç‰ˆæœ¬æ§åˆ¶"
                ]
            },
            "memory_caching": {
                "status": "å·²å®ç°",
                "cache_size": "éœ€è¦ç›‘æ§",
                "suggestions": [
                    "ç›‘æ§å†…å­˜ç¼“å­˜ä½¿ç”¨ç‡",
                    "é…ç½®ç¼“å­˜å¤§å°é™åˆ¶",
                    "å®ç°ç¼“å­˜æ¸…ç†ç­–ç•¥"
                ]
            }
        }
        
        logger.info("âœ“ ç¼“å­˜ç­–ç•¥ä¼˜åŒ–åˆ†æå®Œæˆ")
        return cache_analysis
    
    def generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºå“åº”æ—¶é—´åˆ†æçš„å»ºè®®
        if "api_performance_analysis" in results:
            slow_endpoints = []
            for endpoint, metrics in results["api_performance_analysis"].items():
                if metrics.get("avg_response_time", 0) > 1000:  # è¶…è¿‡1ç§’
                    slow_endpoints.append(endpoint)
            
            if slow_endpoints:
                recommendations.append(f"ä¼˜åŒ–æ…¢å“åº”ç«¯ç‚¹: {', '.join(slow_endpoints)}")
        
        # åŸºäºå¹¶å‘æµ‹è¯•çš„å»ºè®®
        if "concurrency_tests" in results:
            for level, metrics in results["concurrency_tests"].items():
                if metrics.get("success_rate", 100) < 95:
                    recommendations.append(f"æé«˜{level}å¹¶å‘çº§åˆ«çš„æˆåŠŸç‡ï¼Œå½“å‰ä»…{metrics['success_rate']:.1f}%")
        
        # é€šç”¨ä¼˜åŒ–å»ºè®®
        recommendations.extend([
            "å¯ç”¨HTTPå“åº”å‹ç¼©ä»¥å‡å°‘ä¼ è¾“æ—¶é—´",
            "å®ç°APIå“åº”ç¼“å­˜ç­–ç•¥",
            "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œç´¢å¼•",
            "é…ç½®è¿æ¥æ± ä»¥æé«˜å¹¶å‘å¤„ç†èƒ½åŠ›",
            "å®ç°APIé™æµå’Œç†”æ–­æœºåˆ¶",
            "æ·»åŠ APIæ€§èƒ½ç›‘æ§å’Œå‘Šè­¦",
            "ä½¿ç”¨å¼‚æ­¥å¤„ç†é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡",
            "ä¼˜åŒ–JSONåºåˆ—åŒ–æ€§èƒ½",
            "å®ç°æ¡ä»¶è¯·æ±‚æ”¯æŒ(ETag/Last-Modified)",
            "é…ç½®é€‚å½“çš„HTTPç¼“å­˜å¤´"
        ])
        
        return recommendations
    
    def run_optimization(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„APIæ€§èƒ½ä¼˜åŒ–åˆ†æ"""
        logger.info("ğŸš€ å¼€å§‹APIæ¥å£æ€§èƒ½ä¼˜åŒ–åˆ†æ...")
        
        try:
            # 1. APIå“åº”æ—¶é—´åˆ†æ
            self.optimization_results["api_performance_analysis"] = self.analyze_api_response_times()
            
            # 2. å¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•
            self.optimization_results["concurrency_tests"] = self.test_concurrency_performance()
            
            # 3. ä¸­é—´ä»¶æ€§èƒ½åˆ†æ
            self.optimization_results["middleware_optimization"] = self.analyze_middleware_performance()
            
            # 4. è·¯ç”±é…ç½®ä¼˜åŒ–
            self.optimization_results["route_optimization"] = self.analyze_route_optimization()
            
            # 5. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
            self.optimization_results["cache_optimization"] = self.analyze_cache_optimization()
            
            # 6. ç”Ÿæˆä¼˜åŒ–å»ºè®®
            self.optimization_results["recommendations"] = self.generate_recommendations(self.optimization_results)
            
            # 7. ç”Ÿæˆæ€»ç»“
            total_tasks = 5
            completed_tasks = sum(1 for key in ["api_performance_analysis", "concurrency_tests", 
                                              "middleware_optimization", "route_optimization", 
                                              "cache_optimization"] if self.optimization_results.get(key))
            
            self.optimization_results["summary"] = {
                "total_tasks": total_tasks,
                "completed_tasks": completed_tasks,
                "success_rate": (completed_tasks / total_tasks) * 100,
                "optimization_completed_at": datetime.now().isoformat()
            }
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            report_file = "api_optimization_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(self.optimization_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… APIä¼˜åŒ–åˆ†æå®Œæˆï¼æˆåŠŸç‡: {(completed_tasks / total_tasks) * 100:.1f}%")
            logger.info(f"ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
            return self.optimization_results
            
        except Exception as e:
            logger.error(f"âŒ APIä¼˜åŒ–åˆ†æå¤±è´¥: {str(e)}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
        optimizer = APIPerformanceOptimizer()
        
        # è¿è¡Œä¼˜åŒ–åˆ†æ
        results = optimizer.run_optimization()
        
        # è¾“å‡ºç®€è¦ç»“æœ
        print(f"\nâœ… APIæ¥å£æ€§èƒ½ä¼˜åŒ–å®Œæˆï¼æˆåŠŸç‡: {results['summary']['success_rate']:.1f}%")
        print(f"ğŸ“Š åˆ†æäº† {len(results.get('api_performance_analysis', {}))} ä¸ªAPIç«¯ç‚¹")
        print(f"ğŸ”§ ç”Ÿæˆäº† {len(results.get('recommendations', []))} æ¡ä¼˜åŒ–å»ºè®®")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())