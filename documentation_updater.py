#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£å’Œéƒ¨ç½²æŒ‡å—æ›´æ–°ä¼˜åŒ–å·¥å…·

åŠŸèƒ½:
1. åˆ†æç°æœ‰æ–‡æ¡£ç»“æ„å’Œå†…å®¹
2. æ£€æŸ¥æ–‡æ¡£å®Œæ•´æ€§å’Œä¸€è‡´æ€§
3. æ›´æ–°APIæ–‡æ¡£å’Œç”¨æˆ·æŒ‡å—
4. ç”Ÿæˆéƒ¨ç½²æŒ‡å—å’Œé…ç½®æ–‡æ¡£
5. åˆ›å»ºæ–‡æ¡£æ›´æ–°æŠ¥å‘Š

ä½œè€…: ç³»ç»Ÿä¼˜åŒ–å›¢é˜Ÿ
æ—¥æœŸ: 2025-01-04
ç‰ˆæœ¬: 1.0.0
"""

import os
import json
import re
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
from dataclasses import dataclass

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('documentation_update.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DocumentInfo:
    """æ–‡æ¡£ä¿¡æ¯æ•°æ®ç±»"""
    path: str
    name: str
    size: int
    last_modified: datetime
    content_type: str
    completeness_score: float
    issues: List[str]

@dataclass
class DocumentationReport:
    """æ–‡æ¡£æ›´æ–°æŠ¥å‘Šæ•°æ®ç±»"""
    total_documents: int
    updated_documents: int
    new_documents: int
    issues_found: int
    issues_fixed: int
    completeness_score: float
    recommendations: List[str]
    timestamp: datetime

class DocumentationAnalyzer:
    """æ–‡æ¡£åˆ†æå™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.documents = []
        self.issues = []
        
    def analyze_documents(self) -> List[DocumentInfo]:
        """åˆ†æé¡¹ç›®ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        logger.info("å¼€å§‹åˆ†æé¡¹ç›®æ–‡æ¡£...")
        
        # æ–‡æ¡£æ–‡ä»¶æ¨¡å¼
        doc_patterns = [
            '*.md', '*.rst', '*.txt', '*.pdf',
            '*.docx', '*.html', '*.xml'
        ]
        
        documents = []
        
        for pattern in doc_patterns:
            for doc_path in self.project_root.rglob(pattern):
                if self._should_analyze_file(doc_path):
                    doc_info = self._analyze_single_document(doc_path)
                    if doc_info:
                        documents.append(doc_info)
        
        self.documents = documents
        logger.info(f"åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        return documents
    
    def _should_analyze_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ†æè¯¥æ–‡ä»¶"""
        # æ’é™¤çš„ç›®å½•
        exclude_dirs = {
            '.git', '__pycache__', 'node_modules', '.pytest_cache',
            'venv', '.venv', 'env', '.env', 'build', 'dist'
        }
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤ç›®å½•ä¸­
        for part in file_path.parts:
            if part in exclude_dirs:
                return False
        
        return True
    
    def _analyze_single_document(self, doc_path: Path) -> Optional[DocumentInfo]:
        """åˆ†æå•ä¸ªæ–‡æ¡£"""
        try:
            stat = doc_path.stat()
            
            # è¯»å–æ–‡æ¡£å†…å®¹
            content = ""
            if doc_path.suffix.lower() in ['.md', '.rst', '.txt', '.html', '.xml']:
                try:
                    with open(doc_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    try:
                        with open(doc_path, 'r', encoding='gbk') as f:
                            content = f.read()
                    except:
                        content = "[æ— æ³•è¯»å–å†…å®¹]"
            
            # åˆ†ææ–‡æ¡£å®Œæ•´æ€§
            completeness_score, issues = self._analyze_completeness(content, doc_path)
            
            return DocumentInfo(
                path=str(doc_path),
                name=doc_path.name,
                size=stat.st_size,
                last_modified=datetime.fromtimestamp(stat.st_mtime),
                content_type=self._get_content_type(doc_path),
                completeness_score=completeness_score,
                issues=issues
            )
            
        except Exception as e:
            logger.error(f"åˆ†ææ–‡æ¡£ {doc_path} æ—¶å‡ºé”™: {e}")
            return None
    
    def _analyze_completeness(self, content: str, doc_path: Path) -> tuple[float, List[str]]:
        """åˆ†ææ–‡æ¡£å®Œæ•´æ€§"""
        issues = []
        score = 100.0
        
        # æ£€æŸ¥æ–‡æ¡£é•¿åº¦
        if len(content) < 100:
            issues.append("æ–‡æ¡£å†…å®¹è¿‡çŸ­")
            score -= 20
        
        # æ£€æŸ¥æ ‡é¢˜ç»“æ„
        if doc_path.suffix.lower() == '.md':
            if not re.search(r'^#\s+', content, re.MULTILINE):
                issues.append("ç¼ºå°‘ä¸»æ ‡é¢˜")
                score -= 15
            
            # æ£€æŸ¥ä»£ç å—
            code_blocks = re.findall(r'```[\s\S]*?```', content)
            if len(code_blocks) == 0 and 'README' in doc_path.name.upper():
                issues.append("READMEæ–‡æ¡£ç¼ºå°‘ä»£ç ç¤ºä¾‹")
                score -= 10
        
        # æ£€æŸ¥é“¾æ¥æœ‰æ•ˆæ€§
        links = re.findall(r'\[([^\]]+)\]\(([^\)]+)\)', content)
        for link_text, link_url in links:
            if link_url.startswith('http'):
                # å¤–éƒ¨é“¾æ¥æš‚ä¸æ£€æŸ¥
                continue
            elif link_url.startswith('/'):
                # ç»å¯¹è·¯å¾„
                full_path = self.project_root / link_url.lstrip('/')
                if not full_path.exists():
                    issues.append(f"é“¾æ¥ç›®æ ‡ä¸å­˜åœ¨: {link_url}")
                    score -= 5
            else:
                # ç›¸å¯¹è·¯å¾„
                full_path = doc_path.parent / link_url
                if not full_path.exists():
                    issues.append(f"é“¾æ¥ç›®æ ‡ä¸å­˜åœ¨: {link_url}")
                    score -= 5
        
        return max(0, score), issues
    
    def _get_content_type(self, doc_path: Path) -> str:
        """è·å–æ–‡æ¡£å†…å®¹ç±»å‹"""
        name_lower = doc_path.name.lower()
        
        if 'readme' in name_lower:
            return 'READMEæ–‡æ¡£'
        elif 'api' in name_lower:
            return 'APIæ–‡æ¡£'
        elif 'user' in name_lower or 'guide' in name_lower:
            return 'ç”¨æˆ·æŒ‡å—'
        elif 'deploy' in name_lower or 'éƒ¨ç½²' in name_lower:
            return 'éƒ¨ç½²æ–‡æ¡£'
        elif 'config' in name_lower or 'é…ç½®' in name_lower:
            return 'é…ç½®æ–‡æ¡£'
        elif doc_path.suffix.lower() == '.md':
            return 'Markdownæ–‡æ¡£'
        else:
            return 'å…¶ä»–æ–‡æ¡£'

class DocumentationUpdater:
    """æ–‡æ¡£æ›´æ–°å™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.updated_count = 0
        self.new_count = 0
        self.fixed_issues = 0
        
    def update_readme(self) -> bool:
        """æ›´æ–°READMEæ–‡æ¡£"""
        logger.info("æ›´æ–°READMEæ–‡æ¡£...")
        
        readme_path = self.project_root / 'README.md'
        if not readme_path.exists():
            logger.warning("README.mdæ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        try:
            # è¯»å–ç°æœ‰å†…å®¹
            with open(readme_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ›´æ–°å¾½ç« 
            updated_content = self._update_badges(content)
            
            # æ›´æ–°å®‰è£…è¯´æ˜
            updated_content = self._update_installation_guide(updated_content)
            
            # æ›´æ–°APIæ–‡æ¡£é“¾æ¥
            updated_content = self._update_api_links(updated_content)
            
            # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œå†™å…¥æ–‡ä»¶
            if updated_content != content:
                with open(readme_path, 'w', encoding='utf-8') as f:
                    f.write(updated_content)
                self.updated_count += 1
                logger.info("README.mdå·²æ›´æ–°")
            
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°READMEæ—¶å‡ºé”™: {e}")
            return False
    
    def _update_badges(self, content: str) -> str:
        """æ›´æ–°å¾½ç« ä¿¡æ¯"""
        # æ›´æ–°Pythonç‰ˆæœ¬å¾½ç« 
        content = re.sub(
            r'\[!\[Python [0-9.]+\].*?\]\(.*?\)',
            '[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)',
            content
        )
        
        # æ·»åŠ æœ€åæ›´æ–°æ—¶é—´
        current_date = datetime.now().strftime('%Y-%m-%d')
        if 'æœ€åæ›´æ–°' not in content:
            badge_section = f"\n[![Last Updated](https://img.shields.io/badge/last%20updated-{current_date}-green.svg)]()\n"
            # åœ¨ç¬¬ä¸€ä¸ªæ¢è¡Œåæ’å…¥
            lines = content.split('\n')
            if len(lines) > 1:
                lines.insert(1, badge_section.strip())
                content = '\n'.join(lines)
        
        return content
    
    def _update_installation_guide(self, content: str) -> str:
        """æ›´æ–°å®‰è£…æŒ‡å—"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å®‰è£…éƒ¨åˆ†
        if '## å®‰è£…' not in content and '## Installation' not in content:
            # æ·»åŠ å®‰è£…éƒ¨åˆ†
            installation_section = """
## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PostgreSQL 12+
- Redis 6+
- Node.js 16+ (å‰ç«¯å¼€å‘)

### å®‰è£…æ­¥éª¤

1. **å…‹éš†é¡¹ç›®**
```bash
git clone <repository-url>
cd Sys_Rev_Tec
```

2. **å®‰è£…ä¾èµ–**
```bash
# åç«¯ä¾èµ–
pip install -r requirements.txt

# AIæœåŠ¡ä¾èµ–
pip install -r requirements-ai.txt

# å‰ç«¯ä¾èµ–
cd frontend
npm install
```

3. **é…ç½®ç¯å¢ƒ**
```bash
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶é…ç½®æ•°æ®åº“ç­‰ä¿¡æ¯
```

4. **åˆå§‹åŒ–æ•°æ®åº“**
```bash
alembic upgrade head
python scripts/init_db.py
```

5. **å¯åŠ¨æœåŠ¡**
```bash
# å¯åŠ¨ä¸»åº”ç”¨
uvicorn app.main:app --host 0.0.0.0 --port 8000

# å¯åŠ¨AIæœåŠ¡
cd ai_service
uvicorn main:app --host 0.0.0.0 --port 8001

# å¯åŠ¨å‰ç«¯ï¼ˆå¼€å‘æ¨¡å¼ï¼‰
cd frontend
npm run dev
```
"""
            
            # åœ¨ç‰¹æ€§éƒ¨åˆ†åæ’å…¥
            content = re.sub(
                r'(## âœ¨ ç‰¹æ€§.*?\n\n)',
                r'\1' + installation_section + '\n',
                content,
                flags=re.DOTALL
            )
        
        return content
    
    def _update_api_links(self, content: str) -> str:
        """æ›´æ–°APIæ–‡æ¡£é“¾æ¥"""
        # ç¡®ä¿æœ‰APIæ–‡æ¡£é“¾æ¥
        if 'APIæ–‡æ¡£' not in content and 'API Documentation' not in content:
            api_section = """
## ğŸ“š æ–‡æ¡£

- [APIæ–‡æ¡£](http://localhost:8000/docs) - Swagger UI
- [ç”¨æˆ·æŒ‡å—](docs/USER_GUIDE.md) - è¯¦ç»†ä½¿ç”¨è¯´æ˜
- [éƒ¨ç½²æŒ‡å—](README_Docker.md) - Dockeréƒ¨ç½²è¯´æ˜
- [å¼€å‘æ–‡æ¡£](DEVELOPMENT.md) - å¼€å‘ç¯å¢ƒé…ç½®
"""
            
            # åœ¨é¡¹ç›®ç»“æ„å‰æ’å…¥
            content = re.sub(
                r'(## ğŸ“ é¡¹ç›®ç»“æ„)',
                api_section + '\n\1',
                content
            )
        
        return content
    
    def create_deployment_guide(self) -> bool:
        """åˆ›å»ºéƒ¨ç½²æŒ‡å—"""
        logger.info("åˆ›å»ºéƒ¨ç½²æŒ‡å—...")
        
        deployment_guide = """
# ç³»ç»Ÿéƒ¨ç½²æŒ‡å—

æœ¬æ–‡æ¡£æä¾›äº†æ”¿åºœé‡‡è´­é¡¹ç›®å®¡æŸ¥åˆ†æç³»ç»Ÿçš„è¯¦ç»†éƒ¨ç½²æŒ‡å—ã€‚

## ğŸ—ï¸ éƒ¨ç½²æ¶æ„

### ç”Ÿäº§ç¯å¢ƒæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   è´Ÿè½½å‡è¡¡å™¨    â”‚â”€â”€â”€â–¶â”‚   WebæœåŠ¡å™¨     â”‚â”€â”€â”€â–¶â”‚   åº”ç”¨æœåŠ¡å™¨    â”‚
â”‚   (Nginx)       â”‚    â”‚   (Nginx)       â”‚    â”‚   (FastAPI)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ•°æ®åº“æœåŠ¡    â”‚    â”‚   ç¼“å­˜æœåŠ¡      â”‚    â”‚   AIæœåŠ¡æ¨¡å—    â”‚
â”‚  (PostgreSQL)   â”‚    â”‚   (Redis)       â”‚    â”‚  (FastAPI)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ³ Dockeréƒ¨ç½²

### 1. ä½¿ç”¨Docker Composeï¼ˆæ¨èï¼‰

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository-url>
cd Sys_Rev_Tec

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶

# 3. æ„å»ºå’Œå¯åŠ¨æœåŠ¡
docker-compose up -d

# 4. åˆå§‹åŒ–æ•°æ®åº“
docker-compose exec app alembic upgrade head
docker-compose exec app python scripts/init_db.py
```

### 2. æ‰‹åŠ¨Dockeréƒ¨ç½²

```bash
# æ„å»ºé•œåƒ
docker build -t sys-rev-tec:latest .

# å¯åŠ¨PostgreSQL
docker run -d --name postgres \
  -e POSTGRES_DB=sys_rev_tech \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=your_password \
  -p 5432:5432 \
  postgres:13

# å¯åŠ¨Redis
docker run -d --name redis \
  -p 6379:6379 \
  redis:6-alpine

# å¯åŠ¨åº”ç”¨
docker run -d --name sys-rev-tec \
  --link postgres:postgres \
  --link redis:redis \
  -p 8000:8000 \
  -e DATABASE_URL=postgresql://postgres:your_password@postgres:5432/sys_rev_tech \
  -e REDIS_URL=redis://redis:6379/0 \
  sys-rev-tec:latest
```

## ğŸ–¥ï¸ ä¼ ç»Ÿéƒ¨ç½²

### ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+ / CentOS 8+ / RHEL 8+
- **Python**: 3.8+
- **æ•°æ®åº“**: PostgreSQL 12+
- **ç¼“å­˜**: Redis 6+
- **WebæœåŠ¡å™¨**: Nginx 1.18+
- **å†…å­˜**: æœ€å°‘4GBï¼Œæ¨è8GB+
- **å­˜å‚¨**: æœ€å°‘50GBï¼Œæ¨è100GB+

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install -y python3.8 python3.8-venv python3.8-dev
sudo apt install -y postgresql postgresql-contrib
sudo apt install -y redis-server
sudo apt install -y nginx

# CentOS/RHEL
sudo yum update
sudo yum install -y python38 python38-devel
sudo yum install -y postgresql postgresql-server postgresql-contrib
sudo yum install -y redis
sudo yum install -y nginx
```

### 2. æ•°æ®åº“é…ç½®

```bash
# å¯åŠ¨PostgreSQL
sudo systemctl start postgresql
sudo systemctl enable postgresql

# åˆ›å»ºæ•°æ®åº“å’Œç”¨æˆ·
sudo -u postgres psql
CREATE DATABASE sys_rev_tech;
CREATE USER sys_user WITH PASSWORD 'your_secure_password';
GRANT ALL PRIVILEGES ON DATABASE sys_rev_tech TO sys_user;
\q
```

### 3. åº”ç”¨éƒ¨ç½²

```bash
# åˆ›å»ºåº”ç”¨ç›®å½•
sudo mkdir -p /opt/sys_rev_tec
sudo chown $USER:$USER /opt/sys_rev_tec
cd /opt/sys_rev_tec

# å…‹éš†ä»£ç 
git clone <repository-url> .

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.8 -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
pip install -r requirements-ai.txt

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶

# åˆå§‹åŒ–æ•°æ®åº“
alembic upgrade head
python scripts/init_db.py
```

### 4. ç³»ç»ŸæœåŠ¡é…ç½®

åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶ï¼š

```bash
# ä¸»åº”ç”¨æœåŠ¡
sudo tee /etc/systemd/system/sys-rev-tec.service > /dev/null <<EOF
[Unit]
Description=Sys Rev Tec Main Application
After=network.target postgresql.service redis.service

[Service]
Type=exec
User=www-data
Group=www-data
WorkingDirectory=/opt/sys_rev_tec
Environment=PATH=/opt/sys_rev_tec/venv/bin
ExecStart=/opt/sys_rev_tec/venv/bin/uvicorn app.main:app --host 0.0.0.0 --port 8000
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
EOF

# AIæœåŠ¡
sudo tee /etc/systemd/system/sys-rev-tec-ai.service > /dev/null <<EOF
[Unit]
Description=Sys Rev Tec AI Service
After=network.target postgresql.service redis.service

[Service]
Type=exec
User=www-data
Group=www-data
WorkingDirectory=/opt/sys_rev_tec/ai_service
Environment=PATH=/opt/sys_rev_tec/venv/bin
ExecStart=/opt/sys_rev_tec/venv/bin/uvicorn main:app --host 0.0.0.0 --port 8001
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
EOF

# å¯åŠ¨æœåŠ¡
sudo systemctl daemon-reload
sudo systemctl enable sys-rev-tec sys-rev-tec-ai
sudo systemctl start sys-rev-tec sys-rev-tec-ai
```

### 5. Nginxé…ç½®

```bash
# åˆ›å»ºNginxé…ç½®
sudo tee /etc/nginx/sites-available/sys-rev-tec > /dev/null <<EOF
server {
    listen 80;
    server_name your-domain.com;
    
    # é‡å®šå‘åˆ°HTTPS
    return 301 https://\$server_name\$request_uri;
}

server {
    listen 443 ssl http2;
    server_name your-domain.com;
    
    # SSLé…ç½®
    ssl_certificate /path/to/your/certificate.crt;
    ssl_certificate_key /path/to/your/private.key;
    
    # ä¸»åº”ç”¨
    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;
    }
    
    # AIæœåŠ¡
    location /ai/ {
        proxy_pass http://127.0.0.1:8001/;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;
    }
    
    # é™æ€æ–‡ä»¶
    location /static/ {
        alias /opt/sys_rev_tec/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
    
    # ä¸Šä¼ æ–‡ä»¶
    location /uploads/ {
        alias /opt/sys_rev_tec/uploads/;
        expires 1d;
    }
}
EOF

# å¯ç”¨ç«™ç‚¹
sudo ln -s /etc/nginx/sites-available/sys-rev-tec /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl restart nginx
```

## ğŸ”§ é…ç½®ä¼˜åŒ–

### PostgreSQLä¼˜åŒ–

```sql
-- postgresql.conf ä¼˜åŒ–é…ç½®
shared_buffers = 256MB
effective_cache_size = 1GB
work_mem = 4MB
maintenance_work_mem = 64MB
max_connections = 100
```

### Redisä¼˜åŒ–

```bash
# redis.conf ä¼˜åŒ–é…ç½®
maxmemory 512mb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
save 60 10000
```

## ğŸ“Š ç›‘æ§é…ç½®

### Prometheusé…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'sys-rev-tec'
    static_configs:
      - targets: ['localhost:8000', 'localhost:8001']
    metrics_path: '/metrics'
    scrape_interval: 5s
```

### Grafanaä»ªè¡¨æ¿

å¯¼å…¥é¢„é…ç½®çš„ä»ªè¡¨æ¿ï¼š
- ç³»ç»Ÿæ€§èƒ½ç›‘æ§
- åº”ç”¨æŒ‡æ ‡ç›‘æ§
- æ•°æ®åº“æ€§èƒ½ç›‘æ§
- AIæœåŠ¡ç›‘æ§

## ğŸ”’ å®‰å…¨é…ç½®

### 1. é˜²ç«å¢™é…ç½®

```bash
# UFWé…ç½®
sudo ufw allow ssh
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw --force enable
```

### 2. SSLè¯ä¹¦

```bash
# ä½¿ç”¨Let's Encrypt
sudo apt install certbot python3-certbot-nginx
sudo certbot --nginx -d your-domain.com
```

### 3. æ•°æ®åº“å®‰å…¨

```bash
# PostgreSQLå®‰å…¨é…ç½®
sudo -u postgres psql
ALTER USER postgres PASSWORD 'strong_password';
\q

# ç¼–è¾‘ pg_hba.conf
sudo nano /etc/postgresql/12/main/pg_hba.conf
# ä¿®æ”¹è®¤è¯æ–¹å¼ä¸º md5
```

## ğŸ”„ å¤‡ä»½ç­–ç•¥

### æ•°æ®åº“å¤‡ä»½

```bash
#!/bin/bash
# backup_db.sh
BACKUP_DIR="/opt/backups"
DATE=$(date +%Y%m%d_%H%M%S)

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½æ•°æ®åº“
pg_dump -h localhost -U sys_user -d sys_rev_tech > $BACKUP_DIR/db_backup_$DATE.sql

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
gzip $BACKUP_DIR/db_backup_$DATE.sql

# åˆ é™¤7å¤©å‰çš„å¤‡ä»½
find $BACKUP_DIR -name "db_backup_*.sql.gz" -mtime +7 -delete
```

### æ–‡ä»¶å¤‡ä»½

```bash
#!/bin/bash
# backup_files.sh
BACKUP_DIR="/opt/backups"
SOURCE_DIR="/opt/sys_rev_tec"
DATE=$(date +%Y%m%d_%H%M%S)

# å¤‡ä»½åº”ç”¨æ–‡ä»¶
tar -czf $BACKUP_DIR/app_backup_$DATE.tar.gz \
    --exclude='venv' \
    --exclude='__pycache__' \
    --exclude='.git' \
    $SOURCE_DIR

# åˆ é™¤30å¤©å‰çš„å¤‡ä»½
find $BACKUP_DIR -name "app_backup_*.tar.gz" -mtime +30 -delete
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æœåŠ¡æ— æ³•å¯åŠ¨**
   - æ£€æŸ¥ç«¯å£å ç”¨ï¼š`sudo netstat -tlnp | grep :8000`
   - æŸ¥çœ‹æœåŠ¡æ—¥å¿—ï¼š`sudo journalctl -u sys-rev-tec -f`

2. **æ•°æ®åº“è¿æ¥å¤±è´¥**
   - æ£€æŸ¥PostgreSQLçŠ¶æ€ï¼š`sudo systemctl status postgresql`
   - æµ‹è¯•è¿æ¥ï¼š`psql -h localhost -U sys_user -d sys_rev_tech`

3. **AIæœåŠ¡å¼‚å¸¸**
   - æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼šç¡®ä¿AIæ¨¡å‹æ–‡ä»¶å­˜åœ¨
   - æŸ¥çœ‹AIæœåŠ¡æ—¥å¿—ï¼š`sudo journalctl -u sys-rev-tec-ai -f`

### æ€§èƒ½è°ƒä¼˜

1. **æ•°æ®åº“æ€§èƒ½**
   - å®šæœŸæ‰§è¡ŒVACUUM ANALYZE
   - ç›‘æ§æ…¢æŸ¥è¯¢æ—¥å¿—
   - ä¼˜åŒ–ç´¢å¼•ç­–ç•¥

2. **åº”ç”¨æ€§èƒ½**
   - è°ƒæ•´workerè¿›ç¨‹æ•°
   - ä¼˜åŒ–ç¼“å­˜ç­–ç•¥
   - ç›‘æ§å†…å­˜ä½¿ç”¨

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚é‡åˆ°éƒ¨ç½²é—®é¢˜ï¼Œè¯·è”ç³»ï¼š
- æŠ€æœ¯æ”¯æŒé‚®ç®±ï¼šsupport@example.com
- æ–‡æ¡£æ›´æ–°ï¼šdocs@example.com
- ç´§æ€¥è”ç³»ï¼šemergency@example.com

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
**æœ€åæ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**ç»´æŠ¤å›¢é˜Ÿ**: ç³»ç»Ÿè¿ç»´å›¢é˜Ÿ
"""
        
        deployment_path = self.project_root / 'DEPLOYMENT.md'
        try:
            with open(deployment_path, 'w', encoding='utf-8') as f:
                f.write(deployment_guide)
            self.new_count += 1
            logger.info("éƒ¨ç½²æŒ‡å—å·²åˆ›å»º")
            return True
        except Exception as e:
            logger.error(f"åˆ›å»ºéƒ¨ç½²æŒ‡å—æ—¶å‡ºé”™: {e}")
            return False
    
    def update_api_documentation(self) -> bool:
        """æ›´æ–°APIæ–‡æ¡£"""
        logger.info("æ›´æ–°APIæ–‡æ¡£...")
        
        api_doc_path = self.project_root / 'docs' / 'API.md'
        if not api_doc_path.exists():
            logger.warning("API.mdæ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        try:
            # è¯»å–ç°æœ‰å†…å®¹
            with open(api_doc_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ·»åŠ AIæœåŠ¡APIæ–‡æ¡£
            if 'AIæœåŠ¡API' not in content:
                ai_api_section = """
## AIæœåŠ¡API

### æ–‡æ¡£åˆ†æ

**ç«¯ç‚¹**: `POST /api/v1/ai/analyze`

**æè¿°**: å¯¹ä¸Šä¼ çš„æ–‡æ¡£è¿›è¡Œæ™ºèƒ½åˆ†æ

**è¯·æ±‚å‚æ•°**:
```json
{
  "document_id": 1,
  "analysis_type": "full",
  "options": {
    "extract_entities": true,
    "sentiment_analysis": true,
    "risk_assessment": true
  }
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "status": "success",
  "analysis_id": "uuid-string",
  "results": {
    "entities": [...],
    "sentiment": "neutral",
    "risk_score": 0.3,
    "summary": "æ–‡æ¡£æ‘˜è¦"
  }
}
```

### æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ

**ç«¯ç‚¹**: `POST /api/v1/ai/generate-report`

**æè¿°**: åŸºäºé¡¹ç›®æ•°æ®ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š

**è¯·æ±‚å‚æ•°**:
```json
{
  "project_id": 1,
  "report_type": "summary",
  "template_id": "default",
  "options": {
    "include_charts": true,
    "format": "pdf"
  }
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "status": "success",
  "report_id": "uuid-string",
  "download_url": "/api/v1/reports/download/uuid-string",
  "generated_at": "2025-01-04T10:30:00Z"
}
```

### è¯­ä¹‰æœç´¢

**ç«¯ç‚¹**: `POST /api/v1/ai/search`

**æè¿°**: åŸºäºè¯­ä¹‰ç†è§£çš„æ™ºèƒ½æœç´¢

**è¯·æ±‚å‚æ•°**:
```json
{
  "query": "æœç´¢å…³é”®è¯",
  "project_id": 1,
  "limit": 10,
  "filters": {
    "document_type": "contract",
    "date_range": {
      "start": "2024-01-01",
      "end": "2024-12-31"
    }
  }
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "status": "success",
  "results": [
    {
      "document_id": 1,
      "title": "æ–‡æ¡£æ ‡é¢˜",
      "relevance_score": 0.95,
      "snippet": "ç›¸å…³å†…å®¹ç‰‡æ®µ"
    }
  ],
  "total": 25
}
```
"""
                content += ai_api_section
                
                with open(api_doc_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                self.updated_count += 1
                logger.info("APIæ–‡æ¡£å·²æ›´æ–°")
            
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°APIæ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return False

class DocumentationReportGenerator:
    """æ–‡æ¡£æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.report_data = {}
    
    def generate_report(self, analyzer: DocumentationAnalyzer, 
                       updater: DocumentationUpdater) -> DocumentationReport:
        """ç”Ÿæˆæ–‡æ¡£æ›´æ–°æŠ¥å‘Š"""
        logger.info("ç”Ÿæˆæ–‡æ¡£æ›´æ–°æŠ¥å‘Š...")
        
        documents = analyzer.documents
        total_issues = sum(len(doc.issues) for doc in documents)
        avg_completeness = sum(doc.completeness_score for doc in documents) / len(documents) if documents else 0
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(documents)
        
        report = DocumentationReport(
            total_documents=len(documents),
            updated_documents=updater.updated_count,
            new_documents=updater.new_count,
            issues_found=total_issues,
            issues_fixed=updater.fixed_issues,
            completeness_score=avg_completeness,
            recommendations=recommendations,
            timestamp=datetime.now()
        )
        
        return report
    
    def _generate_recommendations(self, documents: List[DocumentInfo]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åˆ†ææ–‡æ¡£ç±»å‹åˆ†å¸ƒ
        doc_types = {}
        for doc in documents:
            doc_type = doc.content_type
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        
        # æ£€æŸ¥ç¼ºå¤±çš„æ–‡æ¡£ç±»å‹
        required_types = {'READMEæ–‡æ¡£', 'APIæ–‡æ¡£', 'ç”¨æˆ·æŒ‡å—', 'éƒ¨ç½²æ–‡æ¡£'}
        missing_types = required_types - set(doc_types.keys())
        
        for missing_type in missing_types:
            recommendations.append(f"å»ºè®®æ·»åŠ {missing_type}")
        
        # æ£€æŸ¥æ–‡æ¡£è´¨é‡
        low_quality_docs = [doc for doc in documents if doc.completeness_score < 60]
        if low_quality_docs:
            recommendations.append(f"æœ‰{len(low_quality_docs)}ä¸ªæ–‡æ¡£è´¨é‡è¾ƒä½ï¼Œå»ºè®®æ”¹è¿›")
        
        # æ£€æŸ¥è¿‡æœŸæ–‡æ¡£
        old_docs = [doc for doc in documents 
                   if (datetime.now() - doc.last_modified).days > 90]
        if old_docs:
            recommendations.append(f"æœ‰{len(old_docs)}ä¸ªæ–‡æ¡£è¶…è¿‡90å¤©æœªæ›´æ–°ï¼Œå»ºè®®æ£€æŸ¥")
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å»ºè®®å®šæœŸæ›´æ–°æ–‡æ¡£å†…å®¹",
            "å»ºè®®æ·»åŠ æ›´å¤šä»£ç ç¤ºä¾‹",
            "å»ºè®®å®Œå–„APIæ–‡æ¡£çš„é”™è¯¯ç è¯´æ˜",
            "å»ºè®®æ·»åŠ å¸¸è§é—®é¢˜è§£ç­”(FAQ)éƒ¨åˆ†",
            "å»ºè®®å¢åŠ éƒ¨ç½²å’Œè¿ç»´ç›¸å…³æ–‡æ¡£"
        ])
        
        return recommendations[:10]  # è¿”å›å‰10ä¸ªå»ºè®®
    
    def save_report(self, report: DocumentationReport, output_path: str) -> bool:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            report_data = {
                'documentation_update_report': {
                    'summary': {
                        'total_documents': report.total_documents,
                        'updated_documents': report.updated_documents,
                        'new_documents': report.new_documents,
                        'issues_found': report.issues_found,
                        'issues_fixed': report.issues_fixed,
                        'completeness_score': round(report.completeness_score, 2),
                        'timestamp': report.timestamp.isoformat()
                    },
                    'recommendations': report.recommendations,
                    'status': 'completed'
                }
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹æ–‡æ¡£å’Œéƒ¨ç½²æŒ‡å—æ›´æ–°ä¼˜åŒ–...")
    
    project_root = os.getcwd()
    
    try:
        # 1. åˆ†æç°æœ‰æ–‡æ¡£
        analyzer = DocumentationAnalyzer(project_root)
        documents = analyzer.analyze_documents()
        
        logger.info(f"æ–‡æ¡£åˆ†æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
        
        # 2. æ›´æ–°æ–‡æ¡£
        updater = DocumentationUpdater(project_root)
        
        # æ›´æ–°README
        updater.update_readme()
        
        # åˆ›å»ºéƒ¨ç½²æŒ‡å—
        updater.create_deployment_guide()
        
        # æ›´æ–°APIæ–‡æ¡£
        updater.update_api_documentation()
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        report_generator = DocumentationReportGenerator()
        report = report_generator.generate_report(analyzer, updater)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(project_root, 'documentation_update_report.json')
        report_generator.save_report(report, report_path)
        
        # 4. è¾“å‡ºç»“æœ
        logger.info("=" * 50)
        logger.info("æ–‡æ¡£æ›´æ–°å®Œæˆï¼")
        logger.info(f"æ€»æ–‡æ¡£æ•°: {report.total_documents}")
        logger.info(f"æ›´æ–°æ–‡æ¡£æ•°: {report.updated_documents}")
        logger.info(f"æ–°å»ºæ–‡æ¡£æ•°: {report.new_documents}")
        logger.info(f"å‘ç°é—®é¢˜æ•°: {report.issues_found}")
        logger.info(f"å®Œæ•´æ€§è¯„åˆ†: {report.completeness_score:.1f}/100")
        logger.info("=" * 50)
        
        # è¾“å‡ºå»ºè®®
        if report.recommendations:
            logger.info("æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(report.recommendations, 1):
                logger.info(f"{i}. {rec}")
        
        return True
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£æ›´æ–°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)